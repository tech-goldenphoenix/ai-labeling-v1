import psycopg2
import os
import json
import requests
from typing import Dict, List, Optional, Union, Any
from dataclasses import dataclass, asdict
from enum import Enum
import base64
from io import BytesIO
from PIL import Image
import google.generativeai as genai
from datetime import datetime, timedelta
import uuid
import numpy as np
from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility
import time
from embedding_service import EmbeddingService
import ollama


# Configuration
class ModelProvider(Enum):
    OLLAMA = "ollama"
    GOOGLE = "google"
    BOTH = "both"


@dataclass
class ProductLabel:
    """C·∫•u tr√∫c label s·∫£n ph·∫©m theo file PDF"""
    image_url: str
    image_recipient: List[str]
    target_audience: List[str]
    usage_purpose: List[str]
    occasion: List[str]
    niche_theme: List[str]
    sentiment_tone: List[str]
    message_type: List[str]
    personalization_type: List[str]
    product_type: List[str]
    placement_display_context: List[str]
    design_style: List[str]
    color_aesthetic: List[str]
    trademark_level: str
    main_subject: List[str]
    text: List[str]


@dataclass
class ProductRecord:
    """C·∫•u tr√∫c record ƒë·ªÉ insert v√†o Milvus"""
    id_sanpham: str
    image: str
    date: str
    like: str
    comment: str
    share: str
    link_redirect: str
    platform: str
    name_store: str
    description: str
    metadata: dict
    image_vector: List[float]
    description_vector: List[float]


class IntegratedProductPipeline:
    """Module t√≠ch h·ª£p: Crawl Data ‚Üí Label ‚Üí Insert Milvus v·ªõi GPT OSS 20B"""

    def __init__(self,
                 db_config: Dict[str, str],
                 google_api_key: str,
                 ollama_model: str = "gpt-oss:20b",
                 milvus_host: str = "10.10.4.25",
                 milvus_port: str = "19530",
                 use_gpu: bool = True):
        """
        Kh·ªüi t·∫°o pipeline t√≠ch h·ª£p v·ªõi GPT OSS 20B v√† GPU support

        Args:
            db_config: C·∫•u h√¨nh database PostgreSQL
            google_api_key: API key cho Google Gemini
            ollama_model: Model Ollama ƒë·ªÉ s·ª≠ d·ª•ng (m·∫∑c ƒë·ªãnh: gpt-oss:20b)
            milvus_host: Milvus host
            milvus_port: Milvus port
            use_gpu: S·ª≠ d·ª•ng GPU hay kh√¥ng
        """
        # Database config
        self.db_config = db_config
        self.db_connection = None
        
        # GPU config
        self.use_gpu = use_gpu
        self._setup_gpu_environment()

        # AI Labeler - Updated for GPT OSS 20B
        self.ollama_model = ollama_model
        self.google_client = None

        # Ki·ªÉm tra v√† verify model availability
        self._verify_ollama_model()

        # Kh·ªüi t·∫°o EmbeddingService v·ªõi GPU support
        print("üîß Kh·ªüi t·∫°o Embedding Service v·ªõi GPU support...")
        self.embedding_service = EmbeddingService()
        self.embedding_dim = self.embedding_service.embedding_dim

        # Kh·ªüi t·∫°o Google Gemini
        if google_api_key:
            genai.configure(api_key=google_api_key)
            self.google_client = genai.GenerativeModel('gemini-1.5-pro-latest')

        # Milvus config
        self.milvus_host = milvus_host
        self.milvus_port = milvus_port
        self.collection_name = "product_collection"
        self.collection = None

        # Log model info
        model_info = self.embedding_service.get_model_info()
        print(f"ü§ñ Embedding Model: {model_info['model_name']}")
        print(f"üìä Embedding Dimensions: {model_info['embedding_dimension']}")
        print(f"üîß Device: {model_info['device']}")
        print(f"ü¶æ Ollama Model: {self.ollama_model}")

        # Initialize connections
        self._connect_db()
        self._connect_milvus()
        self._setup_collection()

    def _setup_gpu_environment(self):
        """C·∫•u h√¨nh m√¥i tr∆∞·ªùng GPU"""
        if self.use_gpu:
            try:
                import torch
                if torch.cuda.is_available():
                    gpu_count = torch.cuda.device_count()
                    gpu_name = torch.cuda.get_device_name(0) if gpu_count > 0 else "Unknown"
                    print(f"üöÄ GPU Support: Enabled")
                    print(f"   üî• Device: {gpu_name}")
                    print(f"   üìä GPU Count: {gpu_count}")
                    print(f"   üíæ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
                    
                    # Set GPU environment variables cho Ollama
                    os.environ['CUDA_VISIBLE_DEVICES'] = '0'
                    os.environ['OLLAMA_GPU'] = '1'
                else:
                    print("‚ö†Ô∏è  CUDA kh√¥ng kh·∫£ d·ª•ng, s·ª≠ d·ª•ng CPU")
                    self.use_gpu = False
            except ImportError:
                print("‚ö†Ô∏è  PyTorch kh√¥ng ƒë∆∞·ª£c c√†i ƒë·∫∑t, s·ª≠ d·ª•ng CPU")
                self.use_gpu = False
        else:
            print("üñ•Ô∏è  GPU Support: Disabled (s·ª≠ d·ª•ng CPU)")

    def _verify_ollama_model(self):
        """Ki·ªÉm tra v√† verify model Ollama c√≥ s·∫µn - Fixed version"""
        try:
            print(f"üîç Ki·ªÉm tra model {self.ollama_model}...")
            
            # List available models v·ªõi error handling t·ªët h∆°n
            try:
                available_models = ollama.list()
                print(f"üìã Raw response structure: {type(available_models)}")
                
                # Handle different response structures
                if isinstance(available_models, dict):
                    if 'models' in available_models:
                        model_list = available_models['models']
                    else:
                        model_list = available_models
                else:
                    model_list = available_models
                
                # Extract model names safely
                model_names = []
                if isinstance(model_list, list):
                    for model in model_list:
                        if isinstance(model, dict):
                            # Try different possible keys
                            name = model.get('name') or model.get('model') or model.get('id', 'unknown')
                            model_names.append(name)
                        else:
                            model_names.append(str(model))
                
                print(f"üìã Available models: {model_names}")
                
            except Exception as e:
                print(f"‚ö†Ô∏è  L·ªói list models: {str(e)}")
                print(f"üí° Th·ª≠ pull model tr·ª±c ti·∫øp...")
                model_names = []
            
            # Check if our model is available
            if self.ollama_model not in model_names:
                print(f"‚ö†Ô∏è  Model {self.ollama_model} ch∆∞a c√≥ s·∫µn!")
                print(f"üîÑ ƒêang t·ª± ƒë·ªông pull model {self.ollama_model}...")
                
                try:
                    # Pull model v·ªõi progress tracking
                    print("‚è≥ Pulling model... (c√≥ th·ªÉ m·∫•t v√†i ph√∫t)")
                    ollama.pull(self.ollama_model)
                    print(f"‚úÖ ƒê√£ pull model {self.ollama_model} th√†nh c√¥ng!")
                    
                    # Verify model is working
                    test_response = ollama.generate(
                        model=self.ollama_model,
                        prompt="Test message",
                        options={'num_predict': 10}
                    )
                    print(f"‚úÖ Model {self.ollama_model} ƒë√£ s·∫µn s√†ng v√† ho·∫°t ƒë·ªông!")
                    
                except Exception as pull_error:
                    print(f"‚ùå L·ªói pull model: {str(pull_error)}")
                    print(f"üí° Vui l√≤ng ch·∫°y th·ªß c√¥ng: ollama pull {self.ollama_model}")
                    raise Exception(f"Model {self.ollama_model} kh√¥ng kh·∫£ d·ª•ng")
            else:
                print(f"‚úÖ Model {self.ollama_model} ƒë√£ s·∫µn s√†ng!")
                
                # Quick test
                try:
                    test_response = ollama.generate(
                        model=self.ollama_model,
                        prompt="Hello",
                        options={'num_predict': 5}
                    )
                    print(f"‚úÖ Model test successful!")
                except Exception as test_error:
                    print(f"‚ö†Ô∏è  Model test warning: {str(test_error)}")
                
        except Exception as e:
            print(f"‚ùå L·ªói ki·ªÉm tra model: {str(e)}")
            print(f"üí° Vui l√≤ng ƒë·∫£m b·∫£o:")
            print(f"   1. Ollama ƒëang ch·∫°y: ollama serve")
            print(f"   2. Model ƒë√£ ƒë∆∞·ª£c pull: ollama pull {self.ollama_model}")
            print(f"   3. C√≥ ƒë·ªß VRAM cho model (kho·∫£ng 12-16GB)")

    def _connect_db(self) -> bool:
        """K·∫øt n·ªëi ƒë·∫øn PostgreSQL database"""
        try:
            self.db_connection = psycopg2.connect(**self.db_config)
            print("‚úÖ K·∫øt n·ªëi PostgreSQL th√†nh c√¥ng")
            return True
        except Exception as e:
            print(f"‚ùå L·ªói k·∫øt n·ªëi database: {e}")
            return False

    def _connect_milvus(self):
        """K·∫øt n·ªëi t·ªõi Milvus"""
        try:
            connections.connect(
                alias="default",
                host=self.milvus_host,
                port=self.milvus_port
            )
            print("‚úÖ K·∫øt n·ªëi Milvus th√†nh c√¥ng")
        except Exception as e:
            raise Exception(f"‚ùå L·ªói k·∫øt n·ªëi Milvus: {str(e)}")

    def _create_collection_schema(self):
        """T·∫°o schema cho collection v·ªõi embedding dimensions ƒë·ªông"""
        fields = [
            FieldSchema(name="id_sanpham", dtype=DataType.VARCHAR, is_primary=True, auto_id=False, max_length=100),
            FieldSchema(name="image_vector", dtype=DataType.FLOAT_VECTOR, dim=self.embedding_dim),
            FieldSchema(name="description_vector", dtype=DataType.FLOAT_VECTOR, dim=self.embedding_dim),
            FieldSchema(name="image", dtype=DataType.VARCHAR, max_length=1000),
            FieldSchema(name="description", dtype=DataType.VARCHAR, max_length=5000),
            FieldSchema(name="metadata", dtype=DataType.JSON),
            FieldSchema(name="date", dtype=DataType.VARCHAR, max_length=50),
            FieldSchema(name="like", dtype=DataType.VARCHAR, max_length=20),
            FieldSchema(name="comment", dtype=DataType.VARCHAR, max_length=20),
            FieldSchema(name="share", dtype=DataType.VARCHAR, max_length=20),
            FieldSchema(name="link_redirect", dtype=DataType.VARCHAR, max_length=2000),
            FieldSchema(name="platform", dtype=DataType.VARCHAR, max_length=200),
            FieldSchema(name="name_store", dtype=DataType.VARCHAR, max_length=500)
        ]

        schema = CollectionSchema(
            fields=fields,
            description=f"Collection ch·ª©a th√¥ng tin s·∫£n ph·∫©m v·ªõi embedding {self.embedding_dim}D"
        )
        return schema

    def _setup_collection(self):
        """T·∫°o ho·∫∑c load collection"""
        try:
            if utility.has_collection(self.collection_name):
                self.collection = Collection(self.collection_name)
                print(f"‚úÖ Load collection '{self.collection_name}' th√†nh c√¥ng")
            else:
                schema = self._create_collection_schema()
                self.collection = Collection(self.collection_name, schema)
                self._create_indexes()
                print(f"‚úÖ T·∫°o collection '{self.collection_name}' th√†nh c√¥ng v·ªõi {self.embedding_dim}D vectors")

            self.collection.load()

        except Exception as e:
            raise Exception(f"‚ùå L·ªói setup collection: {str(e)}")

    def _create_indexes(self):
        """T·∫°o index cho vector fields"""
        # Ch·ªçn nlist ph√π h·ª£p v·ªõi embedding dimension
        nlist = min(self.embedding_dim, 1024)

        index_params = {
            "metric_type": "COSINE",
            "index_type": "IVF_FLAT",
            "params": {"nlist": nlist}
        }

        self.collection.create_index(
            field_name="image_vector",
            index_params=index_params,
            index_name="image_vector_index"
        )

        self.collection.create_index(
            field_name="description_vector",
            index_params=index_params,
            index_name="description_vector_index"
        )
        print(f"‚úÖ T·∫°o indexes th√†nh c√¥ng v·ªõi nlist={nlist}")

    # === DUPLICATE CHECK METHODS ===
    def check_id_exists(self, id_sanpham: str) -> bool:
        """
        Ki·ªÉm tra xem ID s·∫£n ph·∫©m ƒë√£ t·ªìn t·∫°i trong Milvus ch∆∞a
        """
        try:
            expr = f'id_sanpham == "{id_sanpham}"'
            results = self.collection.query(
                expr=expr,
                output_fields=["id_sanpham"],
                limit=1
            )
            return len(results) > 0
        except Exception as e:
            print(f"‚ö†Ô∏è  L·ªói ki·ªÉm tra ID t·ªìn t·∫°i {id_sanpham}: {str(e)}")
            return False

    def check_ids_exist_batch(self, id_list: List[str]) -> Dict[str, bool]:
        """Ki·ªÉm tra nhi·ªÅu ID c√πng l√∫c ƒë·ªÉ t·ªëi ∆∞u performance"""
        try:
            if not id_list:
                return {}

            id_conditions = [f'id_sanpham == "{id_val}"' for id_val in id_list]
            expr = " or ".join(id_conditions)

            results = self.collection.query(
                expr=expr,
                output_fields=["id_sanpham"],
                limit=len(id_list)
            )

            existing_ids = {result["id_sanpham"] for result in results}
            return {id_val: id_val in existing_ids for id_val in id_list}

        except Exception as e:
            print(f"‚ö†Ô∏è  L·ªói ki·ªÉm tra batch IDs: {str(e)}")
            return {id_val: False for id_val in id_list}

    def filter_existing_records(self, raw_data_list: List[Dict[str, Any]]) -> tuple:
        """L·ªçc b·ªè c√°c record ƒë√£ t·ªìn t·∫°i trong Milvus"""
        try:
            if not raw_data_list:
                return [], [], 0

            print(f"üîç Ki·ªÉm tra tr√πng l·∫∑p cho {len(raw_data_list)} records...")

            id_list = [record.get('id_sanpham', '') for record in raw_data_list if record.get('id_sanpham')]

            if not id_list:
                return raw_data_list, [], 0

            existence_map = self.check_ids_exist_batch(id_list)

            new_records = []
            existing_records = []

            for record in raw_data_list:
                id_sanpham = record.get('id_sanpham', '')
                if id_sanpham and existence_map.get(id_sanpham, False):
                    existing_records.append(record)
                else:
                    new_records.append(record)

            duplicate_count = len(existing_records)

            print(f"‚úÖ K·∫øt qu·∫£ ki·ªÉm tra tr√πng l·∫∑p:")
            print(f"   üì¶ Records m·ªõi: {len(new_records)}")
            print(f"   üîÑ Records tr√πng l·∫∑p: {duplicate_count}")

            if duplicate_count > 0:
                print(f"   üìã M·ªôt s·ªë ID tr√πng l·∫∑p:")
                for i, record in enumerate(existing_records[:5]):
                    print(f"      {i + 1}. {record.get('id_sanpham', 'unknown')}")
                if duplicate_count > 5:
                    print(f"      ... v√† {duplicate_count - 5} ID kh√°c")

            return new_records, existing_records, duplicate_count

        except Exception as e:
            print(f"‚ùå L·ªói khi l·ªçc records tr√πng l·∫∑p: {str(e)}")
            return raw_data_list, [], 0

    # === CRAWL DATA METHODS ===
    def crawl_data_by_date_range(self, start_date: str, end_date: str, limit: int = 1000) -> List[Dict[str, Any]]:
        """Crawl data t·ª´ database theo kho·∫£ng th·ªùi gian"""
        if not self.db_connection:
            if not self._connect_db():
                return []

        try:
            cursor = self.db_connection.cursor()

            query = """
            SELECT 
                COALESCE(_id, CONCAT('SP_', SUBSTRING(MD5(_id::text), 1, 8))) as id_sanpham,
                COALESCE(original_url, thumb_url, '') as image,
                COALESCE(CAST(created_at_std AS text), CAST(NOW() AS text)) as date,
                COALESCE(CAST("like" AS text), '0') as like,
                COALESCE(CAST(comment AS text), '0') as comment,
                COALESCE(CAST(share AS text), '0') as share,
                COALESCE(final_url, link, '') as link_redirect,
                COALESCE(platform, 'Website') as platform,
                COALESCE(domain, 'unknown') as name_store
            FROM ai_craw.toidispy_full
            WHERE created_at_std BETWEEN %s AND %s
            ORDER BY created_at_std DESC
            LIMIT %s
            """

            cursor.execute(query, (start_date, end_date, limit))
            columns = [desc[0] for desc in cursor.description]
            results = []

            for row in cursor.fetchall():
                row_dict = dict(zip(columns, row))
                results.append(row_dict)

            cursor.close()
            print(f"‚úÖ Crawl ƒë∆∞·ª£c {len(results)} records t·ª´ {start_date} ƒë·∫øn {end_date}")
            return results

        except Exception as e:
            print(f"‚ùå L·ªói khi crawl data: {e}")
            return []
        

    # === LABELING METHODS - UPDATED FOR GPT OSS 20B ===
    def _create_labeling_prompt(self) -> str:
        """T·∫°o prompt chi ti·∫øt ƒë∆∞·ª£c t·ªëi ∆∞u cho GPT OSS 20B"""
        return """
B·∫°n l√† m·ªôt chuy√™n gia ph√¢n t√≠ch s·∫£n ph·∫©m v·ªõi kh·∫£ nƒÉng hi·ªÉu s√¢u v·ªÅ th·ªã tr∆∞·ªùng v√† xu h∆∞·ªõng ng∆∞·ªùi ti√™u d√πng. H√£y ph√¢n t√≠ch h√¨nh ·∫£nh s·∫£n ph·∫©m n√†y v√† ƒë√°nh label theo c√°c ti√™u ch√≠ sau:

**QUAN TR·ªåNG: CH·ªà CH·ªåN 1-3 LABELS CH√çNH V√Ä PH·ª¶ H·ª¢P NH·∫§T CHO M·ªñI TI√äU CH√ç**

**H∆Ø·ªöNG D·∫™N PH√ÇN T√çCH:**
- Quan s√°t k·ªπ thi·∫øt k·∫ø, m√†u s·∫Øc, vƒÉn b·∫£n, v√† b·ªëi c·∫£nh s·ª≠ d·ª•ng
- X√°c ƒë·ªãnh ƒë·ªëi t∆∞·ª£ng ng∆∞·ªùi d√πng ch√≠nh t·ª´ visual cues
- Ph√¢n t√≠ch c·∫£m x√∫c v√† th√¥ng ƒëi·ªáp m√† s·∫£n ph·∫©m truy·ªÅn t·∫£i
- Ch√∫ √Ω ƒë·∫øn ch·∫•t l∆∞·ª£ng v√† phong c√°ch thi·∫øt k·∫ø

**L∆ØU √ù QUAN TR·ªåNG:**
- Khi th·∫•y "Children", h√£y chi ti·∫øt h√≥a th√†nh "Son", "Daughter", "Kids" thay v√¨ d√πng "Children" chung chung
- "3D Rendered" ch·ªâ √°p d·ª•ng khi THI·∫æT K·∫æ B√äN TRONG s·∫£n ph·∫©m c√≥ v·∫ª ƒë∆∞·ª£c t·∫°o b·∫±ng 3D rendering, KH√îNG PH·∫¢I v√¨ ·∫£nh mockup tr√¥ng 3D
- Ph√¢n bi·ªát r√µ r√†ng gi·ªØa mockup presentation v√† design style th·ª±c t·∫ø c·ªßa s·∫£n ph·∫©m
- ∆Øu ti√™n ƒë·ªô ch√≠nh x√°c v√† c·ª• th·ªÉ trong t·ª´ng label

**TI√äU CH√ç ƒê√ÅNH LABEL:**

1. **Image Recipient** (Ng∆∞·ªùi nh·∫≠n - MAX 4 labels ch√≠nh):
   - Thay v√¨ "Children" ‚Üí s·ª≠ d·ª•ng "Son", "Daughter", "Kids" c·ª• th·ªÉ
   - Ch·ªçn ƒë·ªëi t∆∞·ª£ng ch√≠nh v√† r√µ r√†ng nh·∫•t (v√≠ d·ª•: Mom, Dad, Son, Daughter, Wife, Husband)

2. **Target Audience** (Ng∆∞·ªùi mua - MAX 3 labels):
   - Ch·ªçn nh√≥m mua h√†ng ch√≠nh 
   - Ph·∫£i C·ª§ TH·ªÇ, kh√¥ng ƒë∆∞·ª£c chung chung nh∆∞ "Family Members" hay "Friends"
   - V√≠ d·ª• c·ª• th·ªÉ: "From Daughter", "From Son", "From Husband", "From Wife", "From Mother", "From Father", "From Spouse", "From dog owners", "From beer enthusiasts", "From police officers", "From colleagues", etc.

3. **Usage Purpose** (M·ª•c ƒë√≠ch - MAX 3 labels):
   - M·ª•c ƒë√≠ch s·ª≠ d·ª•ng ch√≠nh (Gift, Home Decor, Personal Use, Keepsake, Functional Use)

4. **Occasion** (D·ªãp - MAX 3 labels):
   - Ch·ªâ nh·ªØng d·ªãp ch√≠nh v√† r√µ r√†ng nh·∫•t
   - Ph·∫£i C·ª§ TH·ªÇ: "Mother's Birthday", "Father's Birthday", "Dad's Birthday", "Son's Birthday", "Daughter's Birthday", "Christmas Gift", "Mother's Day", "Father's Day", "Valentine's Day", "Anniversaries", "Pet birthdays", etc.

5. **Niche/Theme** (Ch·ªß ƒë·ªÅ - MAX 3 labels):
   - Ch·ªß ƒë·ªÅ ch√≠nh c·ªßa s·∫£n ph·∫©m (Mother, Father, Police, Beer, Cowgirl, Witch, Pet, Sports, etc.)

6. **Sentiment/Tone** (C·∫£m x√∫c - MAX 3 labels):
   - C·∫£m x√∫c ch√≠nh (Sentimental, Humorous, Elegant, Sophisticated, Playful, Inspirational, etc.)

7. **Message Type** (Lo·∫°i th√¥ng ƒëi·ªáp - MAX 1 label):
   - Ch·ªçn 1 lo·∫°i ph√π h·ª£p nh·∫•t (No quote, Symbolic Message, From-to Signature, Personal Identity)

8. **Personalization Type** (C√° nh√¢n h√≥a - MAX 1 label):
   - Ch·ªçn lo·∫°i c√° nh√¢n h√≥a ch√≠nh (Personalized Name, Non-personalized, Custom Text, etc.)

9. **Product Type** (Lo·∫°i s·∫£n ph·∫©m - MAX 2 labels):
   - Lo·∫°i s·∫£n ph·∫©m c·ª• th·ªÉ (Desk Plaque, Mug, Hoodie, Earrings, Watch, Keychain, Hanging Suncatcher, T-Shirt, etc.)

10. **Placement/Display Context** (B·ªëi c·∫£nh - MAX 2 labels):
    - N∆°i tr∆∞ng b√†y ch√≠nh (Shelf decor, Desk decor, Bedroom display, Window decor, Wearable, etc.)

11. **Design Style** (Phong c√°ch - MAX 4 labels):
    - CH√ö √ù: "3D Rendered" ch·ªâ khi THI·∫æT K·∫æ in l√™n s·∫£n ph·∫©m th·ª±c s·ª± l√† 3D rendered
    - C√°c phong c√°ch kh√°c: Elegant, Vintage, Stained Glass, Floral Motif, Gothic, Minimalist, Abstract, etc.

12. **Color Aesthetic** (M√†u s·∫Øc - MAX 2 labels):
    - M√†u s·∫Øc ch·ªß ƒë·∫°o (Pink, Blue, Gold, Pastel, Black, Purple, Rainbow, Monochrome, etc.)

13. **Trademark Level** (M·ª©c ƒë·ªô th∆∞∆°ng hi·ªáu - 1 label):
    - Ch·ªçn 1 m·ª©c: No TM, Slight TM, TM, TM resemblance

14. **Main Subject** (Ch·ªß th·ªÉ ch√≠nh - MAX 2 labels):
    - ƒê·ªëi t∆∞·ª£ng ch√≠nh trong thi·∫øt k·∫ø (Rose, Butterfly, Truck, Police Badge, Animal, Text Design, etc.)

15. **Text** (N·ªôi dung vƒÉn b·∫£n):
    - Ghi ch√≠nh x√°c to√†n b·ªô vƒÉn b·∫£n xu·∫•t hi·ªán tr√™n s·∫£n ph·∫©m
    - N·∫øu kh√¥ng c√≥ vƒÉn b·∫£n, ghi "No text"

**OUTPUT FORMAT - B·∫ÆT BU·ªòC PH·∫¢I ƒê√öNG ƒê·ªäNH D·∫†NG JSON:**
```json
{
  "image_recipient": ["value1", "value2"],
  "target_audience": ["value1", "value2"], 
  "usage_purpose": ["value1", "value2", "value3"],
  "occasion": ["value1", "value2"],
  "niche_theme": ["value1", "value2"],
  "sentiment_tone": ["value1", "value2"],
  "message_type": ["value1"],
  "personalization_type": ["value1"],
  "product_type": ["value1", "value2"],
  "placement_display_context": ["value1", "value2"],
  "design_style": ["value1", "value2"],
  "color_aesthetic": ["value1", "value2"],
  "trademark_level": "value",
  "main_subject": ["value1", "value2"],
  "text": ["value1", "value2"]
}
```

H√£y ph√¢n t√≠ch c·∫©n th·∫≠n v√† tr·∫£ v·ªÅ k·∫øt qu·∫£ JSON ch√≠nh x√°c.
"""

    def _download_image(self, url: str) -> bytes:
        """Download image t·ª´ URL v·ªõi retry mechanism"""
        max_retries = 3
        for attempt in range(max_retries):
            try:
                response = requests.get(url, timeout=30, headers={
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                })
                response.raise_for_status()
                return response.content
            except Exception as e:
                if attempt == max_retries - 1:
                    raise Exception(f"L·ªói download ·∫£nh sau {max_retries} l·∫ßn th·ª≠: {str(e)}")
                time.sleep(1)

    def _analyze_with_ollama(self, image_url: str) -> Dict:
        """Ph√¢n t√≠ch v·ªõi GPT OSS 20B - T·ªëi ∆∞u h√≥a cho model m·ªõi"""
        try:
            print(f"üîç ƒêang ph√¢n t√≠ch v·ªõi {self.ollama_model}...")
            
            # Download v√† encode image
            image_bytes = self._download_image(image_url)
            image_base64 = base64.b64encode(image_bytes).decode('utf-8')

            prompt = self._create_labeling_prompt()

            # C·∫•u h√¨nh t·ªëi ∆∞u cho GPT OSS 20B (model l·ªõn h∆°n, c·∫ßn parameters kh√°c)
            response = ollama.generate(
                model=self.ollama_model,
                prompt=prompt,
                images=[image_base64],
                options={
                    'temperature': 0.05,  # Gi·∫£m temperature cho k·∫øt qu·∫£ ·ªïn ƒë·ªãnh
                    'top_p': 0.85,       # T·ªëi ∆∞u cho model l·ªõn
                    'num_ctx': 12288,    # Context length l·ªõn h∆°n cho GPT OSS 20B
                    'repeat_penalty': 1.05,  # Gi·∫£m repeat penalty
                    'num_predict': 3072, # TƒÉng prediction tokens
                    'top_k': 30,        # Th√™m top_k constraint
                    'seed': 42          # Fixed seed ƒë·ªÉ reproducible
                }
            )

            content = response['response']
            
            # Parse JSON response v·ªõi error handling t·ªët h∆°n
            json_start = content.find('{')
            json_end = content.rfind('}') + 1

            if json_start != -1 and json_end != -1:
                json_str = content[json_start:json_end]
                try:
                    result = json.loads(json_str)
                    print(f"‚úÖ {self.ollama_model} ph√¢n t√≠ch th√†nh c√¥ng")
                    return result
                except json.JSONDecodeError as e:
                    print(f"‚ö†Ô∏è  L·ªói parse JSON t·ª´ {self.ollama_model}: {str(e)}")
                    print(f"Raw response: {content[:500]}...")
                    raise Exception(f"Invalid JSON response from {self.ollama_model}")
            else:
                raise Exception(f"Kh√¥ng t√¨m th·∫•y JSON trong response t·ª´ {self.ollama_model}")

        except Exception as e:
            raise Exception(f"L·ªói {self.ollama_model} analysis: {str(e)}")

    def _analyze_with_google(self, image_url: str) -> Dict:
        """Ph√¢n t√≠ch v·ªõi Google Gemini"""
        if not self.google_client:
            raise Exception("Google client ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o")

        try:
            print("üîç ƒêang ph√¢n t√≠ch v·ªõi Google Gemini...")
            
            image_bytes = self._download_image(image_url)
            image = Image.open(BytesIO(image_bytes))
            prompt = self._create_labeling_prompt()

            response = self.google_client.generate_content([prompt, image])
            content = response.text

            json_start = content.find('{')
            json_end = content.rfind('}') + 1

            if json_start != -1 and json_end != -1:
                json_str = content[json_start:json_end]
                result = json.loads(json_str)
                print("‚úÖ Google Gemini ph√¢n t√≠ch th√†nh c√¥ng")
                return result
            else:
                raise Exception("Kh√¥ng t√¨m th·∫•y JSON trong response t·ª´ Google")

        except Exception as e:
            raise Exception(f"L·ªói Google analysis: {str(e)}")

    def _merge_results(self, ollama_result: Dict, google_result: Dict) -> Dict:
        """K·∫øt h·ª£p k·∫øt qu·∫£ t·ª´ 2 model v·ªõi logic th√¥ng minh h∆°n"""
        merged = {}
        all_keys = set(ollama_result.keys()) | set(google_result.keys())

        for key in all_keys:
            ollama_values = ollama_result.get(key, [])
            google_values = google_result.get(key, [])

            if isinstance(ollama_values, list) and isinstance(google_values, list):
                # K·∫øt h·ª£p v√† lo·∫°i b·ªè duplicate, ∆∞u ti√™n GPT OSS 20B
                combined = ollama_values + [v for v in google_values if v not in ollama_values]
                merged[key] = combined[:4]  # Limit to max 4 items
            elif isinstance(ollama_values, str) and isinstance(google_values, str):
                # ∆Øu ti√™n GPT OSS 20B cho string values
                merged[key] = ollama_values if ollama_values else google_values
            else:
                merged[key] = ollama_values if ollama_values else google_values

        return merged

    def label_image(self, image_url: str, provider: ModelProvider = ModelProvider.OLLAMA) -> ProductLabel:
        """ƒê√°nh label cho 1 ·∫£nh s·∫£n ph·∫©m - M·∫∑c ƒë·ªãnh d√πng GPT OSS 20B"""
        try:
            if provider == ModelProvider.OLLAMA:
                result = self._analyze_with_ollama(image_url)
            elif provider == ModelProvider.GOOGLE:
                result = self._analyze_with_google(image_url)
            else:  # BOTH
                try:
                    ollama_result = self._analyze_with_ollama(image_url)
                except Exception as e:
                    print(f"‚ö†Ô∏è  GPT OSS 20B failed, fallback to Google: {str(e)}")
                    result = self._analyze_with_google(image_url)
                else:
                    try:
                        google_result = self._analyze_with_google(image_url)
                        result = self._merge_results(ollama_result, google_result)
                    except Exception as e:
                        print(f"‚ö†Ô∏è  Google failed, using GPT OSS 20B only: {str(e)}")
                        result = ollama_result

            return ProductLabel(
                image_url=image_url,
                image_recipient=result.get('image_recipient', []),
                target_audience=result.get('target_audience', []),
                usage_purpose=result.get('usage_purpose', []),
                occasion=result.get('occasion', []),
                niche_theme=result.get('niche_theme', []),
                sentiment_tone=result.get('sentiment_tone', []),
                message_type=result.get('message_type', []),
                personalization_type=result.get('personalization_type', []),
                product_type=result.get('product_type', []),
                placement_display_context=result.get('placement_display_context', []),
                design_style=result.get('design_style', []),
                color_aesthetic=result.get('color_aesthetic', []),
                trademark_level=result.get('trademark_level', 'No TM'),
                main_subject=result.get('main_subject', []),
                text=result.get('text', [])
            )

        except Exception as e:
            raise Exception(f"L·ªói labeling: {str(e)}")

    # === VECTOR GENERATION METHODS ===
    def _generate_vectors(self, text: str, image_url: str = None) -> tuple:
        """
        T·∫°o embedding vectors cho text v√† image s·ª≠ d·ª•ng Jina v4

        Args:
            text: Text description ƒë·ªÉ embedding
            image_url: URL c·ªßa image ƒë·ªÉ embedding

        Returns:
            tuple: (image_vector, text_vector)
        """
        # S·ª≠ d·ª•ng method t·ª´ EmbeddingService
        image_vector, text_vector = self.embedding_service._generate_vectors(
            text=text,
            image_url=image_url
        )

        print(f"‚úÖ T·∫°o embedding th√†nh c√¥ng - Text: {len(text_vector)}D, Image: {len(image_vector)}D")
        return image_vector, text_vector

    def _generate_vectors_batch(self, descriptions: List[str], image_urls: List[str] = None) -> tuple:
        """
        T·∫°o embedding vectors cho nhi·ªÅu text v√† image c√πng l√∫c (hi·ªáu qu·∫£ h∆°n)

        Args:
            descriptions: List text descriptions
            image_urls: List image URLs (optional)

        Returns:
            tuple: (image_vectors_list, text_vectors_list)
        """
        # Batch embedding cho text
        text_vectors = self.embedding_service.embed_texts_batch(
            descriptions,
            normalize=True,
            batch_size=32
        )

        # Batch embedding cho images (n·∫øu c√≥)
        image_vectors = []
        if image_urls:
            for image_url in image_urls:
                if image_url and image_url.strip():
                    img_vec = self.embedding_service.embed_image(image_url, normalize=True)
                else:
                    img_vec = [0.0] * self.embedding_dim
                image_vectors.append(img_vec)
        else:
            image_vectors = [[0.0] * self.embedding_dim] * len(descriptions)

        print(f"‚úÖ T·∫°o batch embedding th√†nh c√¥ng - {len(descriptions)} records")
        return image_vectors, text_vectors

    def _create_description(self, label: ProductLabel) -> str:
        """T·∫°o description d·∫°ng markdown t·ª´ ProductLabel"""

        def format_list(items: List[str]) -> str:
            if not items:
                return "Kh√¥ng x√°c ƒë·ªãnh"
            return ", ".join(items)

        description = f"""# M√¥ T·∫£ S·∫£n Ph·∫©m

## Th√¥ng Tin C∆° B·∫£n
- **Ch·ªß Th·ªÉ Ch√≠nh**: {format_list(label.main_subject)}
- **Lo·∫°i S·∫£n Ph·∫©m**: {format_list(label.product_type)}
- **M·ª©c ƒê·ªô Th∆∞∆°ng Hi·ªáu**: {label.trademark_level}

## ƒê·ªëi T∆∞·ª£ng & M·ª•c ƒê√≠ch
- **Ng∆∞·ªùi Nh·∫≠n**: {format_list(label.image_recipient)}
- **Ng∆∞·ªùi Mua**: {format_list(label.target_audience)}
- **M·ª•c ƒê√≠ch S·ª≠ D·ª•ng**: {format_list(label.usage_purpose)}
- **D·ªãp S·ª≠ D·ª•ng**: {format_list(label.occasion)}

## Ph√¢n Lo·∫°i S·∫£n Ph·∫©m
- **Ch·ªß ƒê·ªÅ/Ng√°ch**: {format_list(label.niche_theme)}
- **C·∫£m X√∫c/T√¥ng ƒêi·ªáu**: {format_list(label.sentiment_tone)}
- **Lo·∫°i Th√¥ng ƒêi·ªáp**: {format_list(label.message_type)}
- **C√° Nh√¢n H√≥a**: {format_list(label.personalization_type)}
- **N·ªôi Dung Ch·ªØ In**: {format_list(label.text)}

## Thi·∫øt K·∫ø & Tr∆∞ng B√†y
- **B·ªëi C·∫£nh Tr∆∞ng B√†y**: {format_list(label.placement_display_context)}
- **Phong C√°ch Thi·∫øt K·∫ø**: {format_list(label.design_style)}
- **Th·∫©m M·ªπ M√†u S·∫Øc**: {format_list(label.color_aesthetic)}

## T√≥m T·∫Øt
{format_list(label.product_type)} n√†y l√† m·ªôt {format_list(label.main_subject)} ƒë∆∞·ª£c thi·∫øt k·∫ø d√†nh cho {format_list(label.image_recipient)}, ph√π h·ª£p cho {format_list(label.occasion)} v·ªõi phong c√°ch {format_list(label.design_style)} v√† t√¥ng m√†u {format_list(label.color_aesthetic)}.
"""
        return description

    # === MAIN PROCESSING METHODS ===
    def process_single_record(self, raw_data: Dict[str, Any],
                              provider: ModelProvider = ModelProvider.OLLAMA) -> ProductRecord:
        """
        X·ª≠ l√Ω 1 record: raw data ‚Üí label ‚Üí vectors ‚Üí ProductRecord

        Args:
            raw_data: Data th√¥ t·ª´ database
            provider: Model provider ƒë·ªÉ labeling

        Returns:
            ProductRecord s·∫µn s√†ng ƒë·ªÉ insert v√†o Milvus
        """
        try:
            image_url = raw_data.get('image', '')
            if not image_url:
                raise Exception("Kh√¥ng c√≥ URL ·∫£nh")

            # 1. Label metadata
            label = self.label_image(image_url, provider)
            metadata = asdict(label)

            # 2. T·∫°o description markdown
            description = self._create_description(label)

            # 3. T·∫°o embedding vectors b·∫±ng Sentence Transformers
            image_vector, description_vector = self._generate_vectors(description, image_url)

            # 4. T·∫°o ProductRecord
            record = ProductRecord(
                id_sanpham=raw_data.get('id_sanpham', f"SP_{uuid.uuid4().hex[:8]}"),
                image_vector=image_vector,
                description_vector=description_vector,
                image=image_url,
                description=description,
                metadata=metadata,
                date=raw_data.get('date', ''),
                like=raw_data.get('like', '0'),
                comment=raw_data.get('comment', '0'),
                share=raw_data.get('share', '0'),
                link_redirect=raw_data.get('link_redirect', ''),
                platform=raw_data.get('platform', ''),
                name_store=raw_data.get('name_store', '')
            )

            return record

        except Exception as e:
            raise Exception(f"L·ªói x·ª≠ l√Ω record {raw_data.get('id_sanpham', 'unknown')}: {str(e)}")

    def process_batch_records(self, raw_data_list: List[Dict[str, Any]],
                              provider: ModelProvider = ModelProvider.OLLAMA) -> List[ProductRecord]:
        """
        X·ª≠ l√Ω nhi·ªÅu records c√πng l√∫c ƒë·ªÉ t·ªëi ∆∞u batch embedding

        Args:
            raw_data_list: List data th√¥ t·ª´ database
            provider: Model provider ƒë·ªÉ labeling

        Returns:
            List ProductRecord s·∫µn s√†ng ƒë·ªÉ insert v√†o Milvus
        """
        try:
            # Chu·∫©n b·ªã data cho batch processing
            records = []
            descriptions = []
            image_urls = []

            # T·∫°o labels v√† descriptions cho t·∫•t c·∫£ records
            for raw_data in raw_data_list:
                try:
                    image_url = raw_data.get('image', '')
                    if not image_url:
                        continue

                    # Label metadata
                    label = self.label_image(image_url, provider)
                    metadata = asdict(label)

                    # T·∫°o description
                    description = self._create_description(label)

                    # T·∫°o record template (ch∆∞a c√≥ vectors)
                    record = ProductRecord(
                        id_sanpham=raw_data.get('id_sanpham', f"SP_{uuid.uuid4().hex[:8]}"),
                        image_vector=[],  # S·∫Ω ƒë∆∞·ª£c fill sau
                        description_vector=[],  # S·∫Ω ƒë∆∞·ª£c fill sau
                        image=image_url,
                        description=description,
                        metadata=metadata,
                        date=raw_data.get('date', ''),
                        like=raw_data.get('like', '0'),
                        comment=raw_data.get('comment', '0'),
                        share=raw_data.get('share', '0'),
                        link_redirect=raw_data.get('link_redirect', ''),
                        platform=raw_data.get('platform', ''),
                        name_store=raw_data.get('name_store', '')
                    )

                    records.append(record)
                    descriptions.append(description)
                    image_urls.append(image_url)

                except Exception as e:
                    print(f"L·ªói x·ª≠ l√Ω record {raw_data.get('id_sanpham', 'unknown')}: {str(e)}")
                    continue

            if not records:
                return []

            # Batch embedding cho t·∫•t c·∫£ descriptions v√† images
            print(f"üîÑ B·∫Øt ƒë·∫ßu batch embedding cho {len(records)} records...")
            image_vectors, text_vectors = self._generate_vectors_batch(descriptions, image_urls)

            # G√°n vectors v√†o records
            for i, record in enumerate(records):
                record.image_vector = image_vectors[i]
                record.description_vector = text_vectors[i]

            print(f"‚úÖ Ho√†n th√†nh batch processing {len(records)} records")
            return records

        except Exception as e:
            print(f"‚ùå L·ªói batch processing: {str(e)}")
            return []

    def insert_record(self, record: ProductRecord) -> str:
        """
        Insert 1 ProductRecord v√†o Milvus

        Args:
            record: ProductRecord ƒë·ªÉ insert

        Returns:
            ID c·ªßa record ƒë√£ insert
        """
        try:
            data = [
                [record.id_sanpham],
                [record.image_vector],
                [record.description_vector],
                [record.image],
                [record.description],
                [record.metadata],
                [record.date],
                [record.like],
                [record.comment],
                [record.share],
                [record.link_redirect],
                [record.platform],
                [record.name_store]
            ]

            mr = self.collection.insert(data)
            self.collection.flush()

            return record.id_sanpham

        except Exception as e:
            raise Exception(f"L·ªói insert record {record.id_sanpham}: {str(e)}")

    def insert_batch_records(self, records: List[ProductRecord]) -> List[str]:
        """
        Insert nhi·ªÅu ProductRecord v√†o Milvus c√πng l√∫c (hi·ªáu qu·∫£ h∆°n)

        Args:
            records: List ProductRecord ƒë·ªÉ insert

        Returns:
            List ID c·ªßa c√°c record ƒë√£ insert
        """
        try:
            if not records:
                return []

            # Chu·∫©n b·ªã data cho batch insert
            ids = [record.id_sanpham for record in records]
            image_vectors = [record.image_vector for record in records]
            description_vectors = [record.description_vector for record in records]
            images = [record.image for record in records]
            descriptions = [record.description for record in records]
            metadatas = [record.metadata for record in records]
            dates = [record.date for record in records]
            likes = [record.like for record in records]
            comments = [record.comment for record in records]
            shares = [record.share for record in records]
            link_redirects = [record.link_redirect for record in records]
            platforms = [record.platform for record in records]
            name_stores = [record.name_store for record in records]

            data = [
                ids,
                image_vectors,
                description_vectors,
                images,
                descriptions,
                metadatas,
                dates,
                likes,
                comments,
                shares,
                link_redirects,
                platforms,
                name_stores
            ]

            mr = self.collection.insert(data)
            self.collection.flush()

            print(f"‚úÖ Batch insert th√†nh c√¥ng {len(records)} records")
            return ids

        except Exception as e:
            raise Exception(f"L·ªói batch insert: {str(e)}")

    def run_pipeline(self, start_date: str, end_date: str,
                     limit: int = 1000,
                     provider: ModelProvider = ModelProvider.OLLAMA,
                     batch_size: int = 10) -> Dict[str, Any]:
        """
        Ch·∫°y pipeline ho√†n ch·ªânh: Crawl ‚Üí Check Duplicates ‚Üí Label ‚Üí Insert

        Args:
            start_date: Ng√†y b·∫Øt ƒë·∫ßu (YYYY-MM-DD)
            end_date: Ng√†y k·∫øt th√∫c (YYYY-MM-DD)
            limit: S·ªë l∆∞·ª£ng record t·ªëi ƒëa
            provider: Model provider ƒë·ªÉ labeling (m·∫∑c ƒë·ªãnh GPT OSS 20B)
            batch_size: S·ªë record x·ª≠ l√Ω m·ªói batch

        Returns:
            Dictionary ch·ª©a th·ªëng k√™ k·∫øt qu·∫£
        """
        print("üöÄ B·∫ÆT ƒê·∫¶U PIPELINE T√çCH H·ª¢P V·ªöI GPT OSS 20B")
        print(f"üìÖ Th·ªùi gian: {start_date} ‚Üí {end_date}")
        print(f"üìä Gi·ªõi h·∫°n: {limit} records")
        print(f"ü§ñ Provider: {provider.value}")
        print(f"ü¶æ Ollama Model: {self.ollama_model}")
        print("-" * 80)

        start_time = time.time()

        # Statistics
        stats = {
            'start_time': datetime.now().isoformat(),
            'ollama_model': self.ollama_model,
            'crawled_count': 0,
            'duplicate_count': 0,
            'processed_count': 0,
            'inserted_count': 0,
            'failed_count': 0,
            'skipped_duplicates': [],
            'inserted_ids': [],
            'failed_records': [],
            'total_time_seconds': 0
        }

        try:
            # STEP 1: Crawl data t·ª´ database
            print("üì• STEP 1: Crawl data t·ª´ database...")
            raw_data_list = self.crawl_data_by_date_range(start_date, end_date, limit)

            if not raw_data_list:
                print("‚ö†Ô∏è  Kh√¥ng c√≥ data ƒë·ªÉ x·ª≠ l√Ω")
                return stats

            stats['crawled_count'] = len(raw_data_list)
            print(f"‚úÖ Crawl ƒë∆∞·ª£c {len(raw_data_list)} records")

            # STEP 2: Check duplicates v√† l·ªçc b·ªè
            print("üîç STEP 2: Ki·ªÉm tra v√† l·ªçc b·ªè records tr√πng l·∫∑p...")
            new_records, existing_records, duplicate_count = self.filter_existing_records(raw_data_list)

            stats['duplicate_count'] = duplicate_count
            stats['skipped_duplicates'] = [record.get('id_sanpham', 'unknown') for record in existing_records]

            if not new_records:
                print("‚ö†Ô∏è  T·∫•t c·∫£ records ƒë√£ t·ªìn t·∫°i trong Milvus, kh√¥ng c√≥ g√¨ ƒë·ªÉ x·ª≠ l√Ω")
                return stats

            print(f"‚úÖ S·∫Ω x·ª≠ l√Ω {len(new_records)} records m·ªõi v·ªõi {self.ollama_model}")

            # STEP 3: Process t·ª´ng record v·ªõi batch
            print(f"üîÑ STEP 3: X·ª≠ l√Ω {len(new_records)} records m·ªõi v·ªõi batch_size={batch_size}")

            for i in range(0, len(new_records), batch_size):
                batch = new_records[i:i + batch_size]
                batch_num = (i // batch_size) + 1
                total_batches = (len(new_records) + batch_size - 1) // batch_size

                print(f"üì¶ Batch {batch_num}/{total_batches}: X·ª≠ l√Ω {len(batch)} records")

                for j, raw_data in enumerate(batch):
                    record_idx = i + j + 1

                    try:
                        print(f"  üîç [{record_idx}/{len(new_records)}] X·ª≠ l√Ω: {raw_data.get('id_sanpham', 'unknown')}")

                        # Process single record: label + vector
                        record = self.process_single_record(raw_data, provider)
                        stats['processed_count'] += 1

                        # Insert v√†o Milvus
                        inserted_id = self.insert_record(record)
                        stats['inserted_count'] += 1
                        stats['inserted_ids'].append(inserted_id)

                        print(f"  ‚úÖ [{record_idx}/{len(new_records)}] Th√†nh c√¥ng: {inserted_id}")

                        # Delay nh·ªè ƒë·ªÉ tr√°nh rate limit
                        time.sleep(0.5)

                    except Exception as e:
                        stats['failed_count'] += 1
                        error_info = {
                            'id_sanpham': raw_data.get('id_sanpham', 'unknown'),
                            'image_url': raw_data.get('image', ''),
                            'error': str(e)
                        }
                        stats['failed_records'].append(error_info)
                        print(f"  ‚ùå [{record_idx}/{len(new_records)}] L·ªói: {str(e)}")
                        continue

                # Log batch progress
                print(f"üì¶ Batch {batch_num}/{total_batches} ho√†n th√†nh")
                print(f"   ‚úÖ Th√†nh c√¥ng: {stats['processed_count']}/{len(new_records)}")
                print(f"   ‚ùå Th·∫•t b·∫°i: {stats['failed_count']}/{len(new_records)}")

        except Exception as e:
            print(f"‚ùå L·ªói nghi√™m tr·ªçng trong pipeline: {str(e)}")

        finally:
            # T√≠nh to√°n th·ªùi gian
            end_time = time.time()
            stats['total_time_seconds'] = round(end_time - start_time, 2)
            stats['end_time'] = datetime.now().isoformat()

            # Log k·∫øt qu·∫£ cu·ªëi c√πng
            print("=" * 80)
            print("üéä PIPELINE HO√ÄN TH√ÄNH!")
            print(f"üìä TH·ªêNG K√ä T·ªîNG K·∫æT:")
            print(f"   ü¶æ Model s·ª≠ d·ª•ng: {self.ollama_model}")
            print(f"   üì• Crawl: {stats['crawled_count']} records")
            print(f"   üîÑ Tr√πng l·∫∑p (b·ªè qua): {stats['duplicate_count']} records")
            print(f"   üÜï Records m·ªõi: {len(new_records) if 'new_records' in locals() else 0} records")
            print(f"   üîÑ X·ª≠ l√Ω: {stats['processed_count']} records")
            print(f"   ‚úÖ Insert th√†nh c√¥ng: {stats['inserted_count']} records")
            print(f"   ‚ùå Th·∫•t b·∫°i: {stats['failed_count']} records")
            print(f"   ‚è±Ô∏è  T·ªïng th·ªùi gian: {stats['total_time_seconds']}s")

            # T√≠nh t·ªâ l·ªá th√†nh c√¥ng tr√™n records m·ªõi (kh√¥ng t√≠nh tr√πng l·∫∑p)
            new_records_count = len(new_records) if 'new_records' in locals() else max(
                stats['crawled_count'] - stats['duplicate_count'], 1)
            success_rate = stats['inserted_count'] / max(new_records_count, 1) * 100
            print(f"   üìà T·ªâ l·ªá th√†nh c√¥ng: {success_rate:.1f}%")

            # Hi·ªÉn th·ªã collection stats
            try:
                total_entities = self.collection.num_entities
                print(f"   üíæ T·ªïng entities trong Milvus: {total_entities}")
            except:
                pass

            print("=" * 80)

            return stats

    def search_similar_products(self, query_vector: List[float],
                                field_name: str = "image_vector",
                                top_k: int = 5):
        """
        T√¨m ki·∫øm s·∫£n ph·∫©m t∆∞∆°ng t·ª±

        Args:
            query_vector: Vector ƒë·ªÉ search
            field_name: Field vector ƒë·ªÉ search ("image_vector" ho·∫∑c "description_vector")
            top_k: S·ªë k·∫øt qu·∫£ tr·∫£ v·ªÅ
        """
        search_params = {
            "metric_type": "COSINE",
            "params": {"nprobe": 10}
        }

        results = self.collection.search(
            data=[query_vector],
            anns_field=field_name,
            param=search_params,
            limit=top_k,
            output_fields=["id_sanpham", "image", "platform", "metadata"]
        )

        return results

    def save_stats_to_json(self, stats: Dict[str, Any], filename: str = None):
        """L∆∞u th·ªëng k√™ v√†o file JSON"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"pipeline_stats_{timestamp}.json"

        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(stats, f, ensure_ascii=False, indent=2)
            print(f"üíæ ƒê√£ l∆∞u th·ªëng k√™ v√†o: {filename}")
        except Exception as e:
            print(f"‚ùå L·ªói l∆∞u file: {e}")

    def close_connections(self):
        """ƒê√≥ng t·∫•t c·∫£ k·∫øt n·ªëi"""
        try:
            if self.db_connection:
                self.db_connection.close()
                print("‚úÖ ƒê√£ ƒë√≥ng k·∫øt n·ªëi PostgreSQL")
        except:
            pass


def main():
    """
    H√†m main ƒë∆∞·ª£c c·∫≠p nh·∫≠t cho GPT OSS 20B
    """
    print("üöÄ KH·ªûI ƒê·ªòNG INTEGRATED PRODUCT PIPELINE - GPT OSS 20B + GOOGLE")
    print("=" * 60)

    # ========== CONFIGURATION ==========
    # Database config
    db_config = {
        'host': '45.79.189.110',
        'database': 'ai_db',
        'user': 'ai_engineer',
        'password': 'StrongPassword123',
        'port': 5432
    }

    # API Keys
    google_api_key = "AIzaSyC0-LkawNpB_krGzPmR6fe7zpFyk476LGY"

    # Ollama config - Updated to GPT OSS 20B
    ollama_model = "gpt-oss:20b"  # Model m·ªõi ƒë∆∞·ª£c c·∫≠p nh·∫≠t

    # Milvus config
    milvus_host = "10.10.4.25"
    milvus_port = "19530"

    # ========== TH·ªúI GIAN CRAWL ==========
    start_date = "2025-07-01"  # YYYY-MM-DD
    end_date = "2025-08-14"   # YYYY-MM-DD

    # ========== C√ÄI ƒê·∫∂T PIPELINE ==========
    limit = 5000  # Gi·ªõi h·∫°n s·ªë record ƒë·ªÉ test
    provider = ModelProvider.OLLAMA  # M·∫∑c ƒë·ªãnh d√πng GPT OSS 20B
    batch_size = 10  # S·ªë record x·ª≠ l√Ω m·ªói batch

    try:
        # Kh·ªüi t·∫°o pipeline
        print("üîß Kh·ªüi t·∫°o pipeline...")
        pipeline = IntegratedProductPipeline(
            db_config=db_config,
            google_api_key=google_api_key,
            ollama_model=ollama_model,
            milvus_host=milvus_host,
            milvus_port=milvus_port
        )

        print("‚úÖ Pipeline kh·ªüi t·∫°o th√†nh c√¥ng!")

        # Ch·∫°y pipeline ch√≠nh
        print(f"üéØ B·∫Øt ƒë·∫ßu crawl data t·ª´ {start_date} ƒë·∫øn {end_date}")

        stats = pipeline.run_pipeline(
            start_date=start_date,
            end_date=end_date,
            limit=limit,
            provider=provider,
            batch_size=batch_size
        )

        # L∆∞u th·ªëng k√™
        pipeline.save_stats_to_json(stats)

        # Hi·ªÉn th·ªã k·∫øt qu·∫£ ng·∫Øn g·ªçn
        print("\nüéä K·∫æT QU·∫¢ CU·ªêI C√ôNG:")
        print(f"ü¶æ Model: {stats.get('ollama_model', 'N/A')}")
        print(f"‚úÖ Th√†nh c√¥ng: {stats['inserted_count']}/{stats['crawled_count']} records")
        print(f"üîÑ Tr√πng l·∫∑p (b·ªè qua): {stats['duplicate_count']} records")
        print(f"‚è±Ô∏è  Th·ªùi gian: {stats['total_time_seconds']}s")

        if stats['inserted_ids']:
            print(f"üì¶ M·ªôt s·ªë ID ƒë√£ insert:")
            for i, record_id in enumerate(stats['inserted_ids'][:5]):
                print(f"   {i + 1}. {record_id}")
            if len(stats['inserted_ids']) > 5:
                print(f"   ... v√† {len(stats['inserted_ids']) - 5} records kh√°c")

        if stats['skipped_duplicates']:
            print(f"üîÑ M·ªôt s·ªë ID tr√πng l·∫∑p (ƒë√£ b·ªè qua):")
            for i, record_id in enumerate(stats['skipped_duplicates'][:5]):
                print(f"   {i + 1}. {record_id}")
            if len(stats['skipped_duplicates']) > 5:
                print(f"   ... v√† {len(stats['skipped_duplicates']) - 5} records kh√°c")

    except Exception as e:
        print(f"‚ùå L·ªñI NGHI√äM TR·ªåNG: {str(e)}")

    finally:
        try:
            pipeline.close_connections()
        except:
            pass

        print("\nüëã Pipeline k·∫øt th√∫c!")


if __name__ == "__main__":
    main()