import torch
import numpy as np
from PIL import Image
import requests
from io import BytesIO
from transformers import AutoModel, AutoProcessor
from sklearn.preprocessing import normalize
import warnings
from typing import List, Union, Optional
import base64

warnings.filterwarnings("ignore")


class JinaV4EmbeddingService:
    """
    Service s·ª≠ d·ª•ng Jina v4 ƒë·ªÉ t·∫°o embedding cho text v√† image
    H·ªó tr·ª£ c·∫£ single v√† batch processing
    """

    def __init__(self, device=None,max_length=8192):
        """
        Kh·ªüi t·∫°o Jina v4 embedding service

        Args:
            device: Device ƒë·ªÉ ch·∫°y model ('cuda', 'cpu', ho·∫∑c None ƒë·ªÉ auto-detect)
        """
        self.device = device if device else ('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"üöÄ Kh·ªüi t·∫°o Jina v4 tr√™n device: {self.device}")
        self.max_length = max_length
        # Load Jina v4 model v√† processor
        self.model_name = "jinaai/jina-clip-v2"

        # X√°c ƒë·ªãnh dtype ph√π h·ª£p v·ªõi device v√† hardware
        if self.device == 'cuda' and torch.cuda.is_available():
            # Ki·ªÉm tra kh·∫£ nƒÉng h·ªó tr·ª£ c·ªßa GPU
            if hasattr(torch.cuda, 'is_bf16_supported') and torch.cuda.is_bf16_supported():
                model_dtype = torch.bfloat16
                print("üîß S·ª≠ d·ª•ng bfloat16 cho GPU")
            else:
                model_dtype = torch.float16
                print("üîß S·ª≠ d·ª•ng float16 cho GPU")
        else:
            model_dtype = torch.float32
            print("üîß S·ª≠ d·ª•ng float32 cho CPU")

        try:
            # Load model v·ªõi dtype ph√π h·ª£p
            self.model = AutoModel.from_pretrained(
                self.model_name,
                trust_remote_code=True,
                torch_dtype=model_dtype
            ).to(self.device)

            self.processor = AutoProcessor.from_pretrained(
                self.model_name,
                trust_remote_code=True
            )

            # ƒê·∫∑t model ·ªü ch·∫ø ƒë·ªô eval
            self.model.eval()

            # L·∫•y embedding dimension
            self.embedding_dim = self._get_embedding_dimension()

            print(f"‚úÖ Load model {self.model_name} th√†nh c√¥ng!")
            print(f"üìä Embedding dimension: {self.embedding_dim}")
            print(f"üîß Model dtype: {model_dtype}")

        except Exception as e:
            print(f"‚ùå L·ªói load model: {e}")
            # Th·ª≠ l·∫°i v·ªõi float32 n·∫øu c√≥ l·ªói dtype
            print("üîÑ Th·ª≠ l·∫°i v·ªõi float32...")
            try:
                self.model = AutoModel.from_pretrained(
                    self.model_name,
                    trust_remote_code=True,
                    torch_dtype=torch.float32
                ).to(self.device)

                self.processor = AutoProcessor.from_pretrained(
                    self.model_name,
                    trust_remote_code=True
                )

                self.model.eval()
                self.embedding_dim = self._get_embedding_dimension()
                print(f"‚úÖ Load model th√†nh c√¥ng v·ªõi float32!")
                print(f"üìä Embedding dimension: {self.embedding_dim}")
            except Exception as e2:
                raise Exception(f"Kh√¥ng th·ªÉ load model: {e2}")

    def _get_embedding_dimension(self):
        """L·∫•y dimension c·ªßa embedding vector v·ªõi error handling t·ªët h∆°n"""
        try:
            # Test v·ªõi m·ªôt text ng·∫Øn ƒë·ªÉ l·∫•y dimension
            test_inputs = self.processor(text=["test"], return_tensors="pt", padding=True, truncation=True,max_length=self.max_length)

            # Chuy·ªÉn inputs sang device
            test_inputs = {k: v.to(self.device) for k, v in test_inputs.items() if isinstance(v, torch.Tensor)}

            with torch.no_grad():
                test_output = self.model.get_text_features(**test_inputs)
                return test_output.shape[-1]
        except Exception as e:
            print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ t·ª± ƒë·ªông detect embedding dimension: {e}")
            return 1024  # Default dimension cho Jina CLIP v2

    def _load_image(self, image_url: str) -> Image.Image:
        """
        Load image t·ª´ URL ho·∫∑c ƒë∆∞·ªùng d·∫´n local

        Args:
            image_url: URL ho·∫∑c ƒë∆∞·ªùng d·∫´n ƒë·∫øn image

        Returns:
            PIL Image object
        """
        try:
            if image_url.startswith(('http://', 'https://')):
                response = requests.get(image_url, timeout=30, headers={
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                })
                response.raise_for_status()
                image = Image.open(BytesIO(response.content))
            else:
                image = Image.open(image_url)

            # Convert sang RGB n·∫øu c·∫ßn
            if image.mode != 'RGB':
                image = image.convert('RGB')

            # Resize image n·∫øu qu√° l·ªõn (t·ªëi ∆∞u performance)
            max_size = 1024
            if max(image.size) > max_size:
                image.thumbnail((max_size, max_size), Image.Resampling.LANCZOS)

            return image

        except Exception as e:
            raise ValueError(f"Kh√¥ng th·ªÉ load image t·ª´ {image_url}: {e}")

    def _safe_model_inference(self, model_fn, **kwargs):
        """
        Safe wrapper cho model inference v·ªõi dtype error handling

        Args:
            model_fn: Model function (get_text_features ho·∫∑c get_image_features)
            **kwargs: Arguments cho model function

        Returns:
            Model output tensor
        """
        try:
            return model_fn(**kwargs)
        except RuntimeError as e:
            if "BFloat16" in str(e) or "unsupported ScalarType" in str(e):
                print("‚ö†Ô∏è Dtype error detected, chuy·ªÉn model sang float32...")
                # Chuy·ªÉn model sang float32
                self.model = self.model.float()
                # Th·ª≠ l·∫°i
                return model_fn(**kwargs)
            else:
                raise e

    def embed_text(self, text: str, normalize_output: bool = True) -> np.ndarray:
        """
        T·∫°o embedding cho text v·ªõi error handling c·∫£i thi·ªán

        Args:
            text: Text c·∫ßn embedding
            normalize_output: C√≥ normalize vector hay kh√¥ng

        Returns:
            numpy array ch·ª©a text embedding
        """
        if not text or not text.strip():
            # Tr·∫£ v·ªÅ zero vector n·∫øu text r·ªóng
            return np.zeros(self.embedding_dim, dtype=np.float32)

        try:
            # Preprocess input
            inputs = self.processor(text=[text], return_tensors="pt", padding=True, truncation=True)

            # Chuy·ªÉn inputs sang device
            inputs = {k: v.to(self.device) for k, v in inputs.items() if isinstance(v, torch.Tensor)}

            with torch.no_grad():
                # Safe inference v·ªõi dtype error handling
                outputs = self._safe_model_inference(self.model.get_text_features, **inputs)
                embedding = outputs.cpu().float().numpy()[0]  # Lu√¥n chuy·ªÉn v·ªÅ float32

            if normalize_output:
                embedding = normalize([embedding])[0]

            return embedding.astype(np.float32)

        except Exception as e:
            print(f"‚ùå L·ªói embedding text: {e}")
            return np.zeros(self.embedding_dim, dtype=np.float32)

    def embed_image(self, image_url: str, normalize_output: bool = True) -> np.ndarray:
        """
        T·∫°o embedding cho image v·ªõi error handling c·∫£i thi·ªán

        Args:
            image_url: URL ho·∫∑c ƒë∆∞·ªùng d·∫´n ƒë·∫øn image
            normalize_output: C√≥ normalize vector hay kh√¥ng

        Returns:
            numpy array ch·ª©a image embedding
        """
        if not image_url or not image_url.strip():
            # Tr·∫£ v·ªÅ zero vector n·∫øu image_url r·ªóng
            return np.zeros(self.embedding_dim, dtype=np.float32)

        try:
            image = self._load_image(image_url)

            # Preprocess input
            inputs = self.processor(images=[image], return_tensors="pt")

            # Chuy·ªÉn inputs sang device
            inputs = {k: v.to(self.device) for k, v in inputs.items() if isinstance(v, torch.Tensor)}

            with torch.no_grad():
                # Safe inference v·ªõi dtype error handling
                outputs = self._safe_model_inference(self.model.get_image_features, **inputs)
                embedding = outputs.cpu().float().numpy()[0]  # Lu√¥n chuy·ªÉn v·ªÅ float32

            if normalize_output:
                embedding = normalize([embedding])[0]

            return embedding.astype(np.float32)

        except Exception as e:
            print(f"‚ùå L·ªói embedding image {image_url}: {e}")
            return np.zeros(self.embedding_dim, dtype=np.float32)

    def embed_multimodal(self, text: str, image_url: str = None, normalize: bool = True) -> tuple:
        """
        T·∫°o embedding cho c·∫£ text v√† image

        Args:
            text: Text c·∫ßn embedding
            image_url: URL ho·∫∑c ƒë∆∞·ªùng d·∫´n ƒë·∫øn image (optional)
            normalize: C√≥ normalize vectors hay kh√¥ng

        Returns:
            tuple: (image_vector, text_vector)
        """
        # T·∫°o text embedding
        text_vector = self.embed_text(text, normalize_output=normalize)

        # T·∫°o image embedding n·∫øu c√≥ image_url
        if image_url and image_url.strip():
            image_vector = self.embed_image(image_url, normalize_output=normalize)
        else:
            # Tr·∫£ v·ªÅ zero vector v·ªõi c√πng dimension n·∫øu kh√¥ng c√≥ image
            image_vector = np.zeros(self.embedding_dim, dtype=np.float32)

        return image_vector, text_vector

    def embed_texts_batch(self, texts: List[str], normalize: bool = True, batch_size: int = 32) -> List[np.ndarray]:
        """
        Batch embedding cho nhi·ªÅu text c√πng l√∫c (hi·ªáu qu·∫£ h∆°n)

        Args:
            texts: List text c·∫ßn embedding
            normalize: C√≥ normalize vectors hay kh√¥ng
            batch_size: K√≠ch th∆∞·ªõc batch

        Returns:
            List numpy arrays ch·ª©a text embeddings
        """
        all_embeddings = []

        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]

            try:
                inputs = self.processor(
                    text=batch_texts,
                    return_tensors="pt",
                    padding=True,
                    truncation=True
                )

                # Chuy·ªÉn inputs sang device
                inputs = {k: v.to(self.device) for k, v in inputs.items() if isinstance(v, torch.Tensor)}

                with torch.no_grad():
                    # Safe inference v·ªõi dtype error handling
                    outputs = self._safe_model_inference(self.model.get_text_features, **inputs)
                    embeddings = outputs.cpu().float().numpy()  # Lu√¥n chuy·ªÉn v·ªÅ float32

                if normalize:
                    embeddings = normalize(embeddings)

                for emb in embeddings:
                    all_embeddings.append(emb.astype(np.float32))

            except Exception as e:
                print(f"‚ùå L·ªói batch embedding texts: {e}")
                # Th√™m zero vectors cho batch b·ªã l·ªói
                for _ in batch_texts:
                    all_embeddings.append(np.zeros(self.embedding_dim, dtype=np.float32))

        return all_embeddings

    def embed_images_batch(self, image_urls: List[str], normalize: bool = True, batch_size: int = 16) -> List[
        np.ndarray]:
        """
        Batch embedding cho nhi·ªÅu image c√πng l√∫c

        Args:
            image_urls: List URL images c·∫ßn embedding
            normalize: C√≥ normalize vectors hay kh√¥ng
            batch_size: K√≠ch th∆∞·ªõc batch (nh·ªè h∆°n text v√¨ image t·ªën memory h∆°n)

        Returns:
            List numpy arrays ch·ª©a image embeddings
        """
        all_embeddings = []

        for i in range(0, len(image_urls), batch_size):
            batch_urls = image_urls[i:i + batch_size]
            batch_images = []

            # Load batch images
            for url in batch_urls:
                try:
                    if url and url.strip():
                        image = self._load_image(url)
                        batch_images.append(image)
                    else:
                        batch_images.append(None)
                except:
                    batch_images.append(None)

            # Process batch
            try:
                valid_images = [img for img in batch_images if img is not None]

                if not valid_images:
                    # T·∫•t c·∫£ images trong batch ƒë·ªÅu invalid
                    for _ in batch_images:
                        all_embeddings.append(np.zeros(self.embedding_dim, dtype=np.float32))
                    continue

                # Preprocess valid images
                inputs = self.processor(images=valid_images, return_tensors="pt")

                # Chuy·ªÉn inputs sang device
                inputs = {k: v.to(self.device) for k, v in inputs.items() if isinstance(v, torch.Tensor)}

                with torch.no_grad():
                    # Safe inference v·ªõi dtype error handling
                    outputs = self._safe_model_inference(self.model.get_image_features, **inputs)
                    embeddings = outputs.cpu().float().numpy()  # Lu√¥n chuy·ªÉn v·ªÅ float32

                if normalize:
                    embeddings = normalize(embeddings)

                # Map embeddings back to original order
                valid_idx = 0
                for img in batch_images:
                    if img is not None:
                        all_embeddings.append(embeddings[valid_idx].astype(np.float32))
                        valid_idx += 1
                    else:
                        all_embeddings.append(np.zeros(self.embedding_dim, dtype=np.float32))

            except Exception as e:
                print(f"‚ùå L·ªói batch embedding images: {e}")
                # Th√™m zero vectors cho batch b·ªã l·ªói
                for _ in batch_images:
                    all_embeddings.append(np.zeros(self.embedding_dim, dtype=np.float32))

        return all_embeddings

    def get_model_info(self) -> dict:
        """
        L·∫•y th√¥ng tin v·ªÅ model

        Returns:
            Dictionary ch·ª©a th√¥ng tin model
        """
        return {
            'model_name': self.model_name,
            'embedding_dimension': self.embedding_dim,
            'device': self.device,
            'torch_dtype': str(self.model.dtype) if hasattr(self, 'model') and hasattr(self.model,
                                                                                       'dtype') else 'unknown'
        }

    def similarity_search(self, query_vector: np.ndarray, candidate_vectors: List[np.ndarray], top_k: int = 5) -> List[
        tuple]:
        """
        T√¨m ki·∫øm similarity gi·ªØa query vector v√† danh s√°ch candidate vectors

        Args:
            query_vector: Vector query
            candidate_vectors: List vectors ƒë·ªÉ so s√°nh
            top_k: S·ªë k·∫øt qu·∫£ top tr·∫£ v·ªÅ

        Returns:
            List tuple (index, similarity_score) ƒë∆∞·ª£c s·∫Øp x·∫øp theo similarity gi·∫£m d·∫ßn
        """
        similarities = []

        for i, candidate in enumerate(candidate_vectors):
            # Cosine similarity
            similarity = np.dot(query_vector, candidate) / (
                    np.linalg.norm(query_vector) * np.linalg.norm(candidate)
            )
            similarities.append((i, float(similarity)))

        # S·∫Øp x·∫øp theo similarity gi·∫£m d·∫ßn
        similarities.sort(key=lambda x: x[1], reverse=True)

        return similarities[:top_k]

    def __del__(self):
        """Cleanup khi object b·ªã destroy"""
        try:
            if hasattr(self, 'model') and self.model is not None:
                del self.model
            if hasattr(self, 'processor') and self.processor is not None:
                del self.processor
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
        except:
            pass


# T√≠ch h·ª£p v√†o class ch√≠nh
class EmbeddingService(JinaV4EmbeddingService):
    """
    Service wrapper ƒë·ªÉ t∆∞∆°ng th√≠ch v·ªõi code hi·ªán t·∫°i
    T√≠ch h·ª£p v·ªõi h√†m _generate_vectors t·ª´ pipeline
    """

    def __init__(self, device=None):
        super().__init__(device)
        print(f"ü§ñ EmbeddingService kh·ªüi t·∫°o v·ªõi Jina v4")
        print(f"üìä Embedding dimensions: {self.embedding_dim}")

    def _generate_vectors(self, text: str, image_url: str = None) -> tuple:
        """
        T·∫°o embedding vectors cho text v√† image s·ª≠ d·ª•ng Jina v4

        H√†m n√†y t∆∞∆°ng th√≠ch v·ªõi pipeline hi·ªán t·∫°i

        Args:
            text: Text description ƒë·ªÉ embedding
            image_url: URL c·ªßa image ƒë·ªÉ embedding

        Returns:
            tuple: (image_vector, text_vector)
        """
        image_vector, text_vector = self.embed_multimodal(
            text=text,
            image_url=image_url,
            normalize=True  # Normalize ƒë·ªÉ t·ªëi ∆∞u cho cosine similarity
        )

        print(f"‚úÖ T·∫°o embedding th√†nh c√¥ng - Text: {len(text_vector)}D, Image: {len(image_vector)}D")
        return image_vector, text_vector

    def _generate_vectors_batch(self, descriptions: List[str], image_urls: List[str] = None) -> tuple:
        """
        T·∫°o embedding vectors cho nhi·ªÅu text v√† image c√πng l√∫c (hi·ªáu qu·∫£ h∆°n)

        H√†m n√†y t∆∞∆°ng th√≠ch v·ªõi pipeline hi·ªán t·∫°i

        Args:
            descriptions: List text descriptions
            image_urls: List image URLs (optional)

        Returns:
            tuple: (image_vectors_list, text_vectors_list)
        """
        # Batch embedding cho text
        text_vectors = self.embed_texts_batch(
            descriptions,
            normalize=True,
            batch_size=32
        )

        # Batch embedding cho images (n·∫øu c√≥)
        if image_urls:
            image_vectors = self.embed_images_batch(
                image_urls,
                normalize=True,
                batch_size=16
            )
        else:
            image_vectors = [np.zeros(self.embedding_dim, dtype=np.float32) for _ in descriptions]

        print(f"‚úÖ T·∫°o batch embedding th√†nh c√¥ng - {len(descriptions)} records")
        return image_vectors, text_vectors