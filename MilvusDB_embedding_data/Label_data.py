import psycopg2
import os
import json
import requests
from typing import Dict, List, Optional, Union, Any, Callable
from dataclasses import dataclass, asdict
from enum import Enum
import base64
from io import BytesIO
from PIL import Image
from datetime import datetime, timedelta
import uuid
import numpy as np
from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility
import time
from embedding_service import EmbeddingService
import ollama
import concurrent.futures
from functools import lru_cache
import threading


# Configuration
class ModelProvider(Enum):
    OLLAMA = "ollama"


@dataclass
class ProductLabel:
    """Cáº¥u trÃºc label sáº£n pháº©m theo file PDF"""
    image_url: str
    image_recipient: List[str]
    target_audience: List[str]
    usage_purpose: List[str]
    occasion: List[str]
    niche_theme: List[str]
    sentiment_tone: List[str]
    message_type: List[str]
    personalization_type: List[str]
    product_type: List[str]
    placement_display_context: List[str]
    design_style: List[str]
    color_aesthetic: List[str]
    trademark_level: str
    main_subject: List[str]
    text: List[str]


@dataclass
class ProductRecord:
    """Cáº¥u trÃºc record Ä‘á»ƒ insert vÃ o Milvus"""
    id_sanpham: str
    image: str
    date: str
    like: str
    comment: str
    share: str
    link_redirect: str
    platform: str
    name_store: str
    description: str
    metadata: dict
    image_vector: List[float]
    description_vector: List[float]


class MoELabeler:
    """Mixture of Experts Labeler cho phÃ¢n tÃ­ch sáº£n pháº©m nhanh"""

    def __init__(self, ollama_model: str = "gpt-oss:20b"):
        self.model = ollama_model
        self.experts = {
            'basic_info': self._create_basic_info_prompt,
            'audience_purpose': self._create_audience_purpose_prompt,
            'design_style': self._create_design_style_prompt,
            'product_classification': self._create_product_classification_prompt
        }

        # Cache cho image data Ä‘á»ƒ trÃ¡nh download nhiá»u láº§n
        self._image_cache = {}
        self._cache_lock = threading.Lock()

    @lru_cache(maxsize=4)
    def _create_basic_info_prompt(self) -> str:
        """Expert 1: ThÃ´ng tin cÆ¡ báº£n cá»§a sáº£n pháº©m"""
        return """
Báº¡n lÃ  expert phÃ¢n tÃ­ch THÃ”NG TIN CÆ  Báº¢N sáº£n pháº©m. HÃ£y táº­p trung vÃ o:

**CHá»ˆ PHÃ‚N TÃCH:**
1. **Product Type** (1-2 labels): Loáº¡i sáº£n pháº©m cá»¥ thá»ƒ (Mug, Hoodie, Watch, Keychain, etc.)
2. **Main Subject** (1-2 labels): Äá»‘i tÆ°á»£ng chÃ­nh trong thiáº¿t káº¿ (Rose, Butterfly, Truck, etc.)
3. **Text** (táº¥t cáº£ text): ToÃ n bá»™ vÄƒn báº£n trÃªn sáº£n pháº©m
4. **Trademark Level** (1 label): No TM, Slight TM, TM, TM resemblance

**OUTPUT JSON:**
```json
{
  "product_type": ["value1", "value2"],
  "main_subject": ["value1", "value2"],
  "text": ["value1", "value2"],
  "trademark_level": "value"
}
```
"""

    @lru_cache(maxsize=4)
    def _create_audience_purpose_prompt(self) -> str:
        """Expert 2: Äá»‘i tÆ°á»£ng vÃ  má»¥c Ä‘Ã­ch"""
        return """
Báº¡n lÃ  expert phÃ¢n tÃ­ch Äá»I TÆ¯á»¢NG VÃ€ Má»¤C ÄÃCH sáº£n pháº©m. HÃ£y táº­p trung vÃ o:

**CHá»ˆ PHÃ‚N TÃCH:**
1. **Image Recipient** (1-3 labels): NgÆ°á»i nháº­n cá»¥ thá»ƒ (Mom, Dad, Son, Daughter - KHÃ”NG dÃ¹ng Children)
2. **Target Audience** (1-3 labels): NgÆ°á»i mua cá»¥ thá»ƒ (From Daughter, From Son, From Wife, etc.)
3. **Usage Purpose** (1-3 labels): Má»¥c Ä‘Ã­ch sá»­ dá»¥ng (Gift, Home Decor, Personal Use, etc.)
4. **Occasion** (1-3 labels): Dá»‹p cá»¥ thá»ƒ (Mother's Birthday, Father's Day, Christmas Gift, etc.)

**OUTPUT JSON:**
```json
{
  "image_recipient": ["value1", "value2"],
  "target_audience": ["value1", "value2"], 
  "usage_purpose": ["value1", "value2"],
  "occasion": ["value1", "value2"]
}
```
"""

    @lru_cache(maxsize=4)
    def _create_design_style_prompt(self) -> str:
        """Expert 3: Thiáº¿t káº¿ vÃ  phong cÃ¡ch"""
        return """
Báº¡n lÃ  expert phÃ¢n tÃ­ch THIáº¾T Káº¾ VÃ€ PHONG CÃCH sáº£n pháº©m. HÃ£y táº­p trung vÃ o:

**CHá»ˆ PHÃ‚N TÃCH:**
1. **Design Style** (1-3 labels): Phong cÃ¡ch (Elegant, Vintage, Stained Glass - CHÃš Ã: 3D Rendered chá»‰ khi thiáº¿t káº¿ thá»±c sá»± lÃ  3D)
2. **Color Aesthetic** (1-2 labels): MÃ u sáº¯c chá»§ Ä‘áº¡o (Pink, Blue, Gold, Pastel, etc.)
3. **Placement Display Context** (1-2 labels): Bá»‘i cáº£nh trÆ°ng bÃ y (Shelf decor, Desk decor, etc.)
4. **Sentiment Tone** (1-2 labels): Cáº£m xÃºc (Sentimental, Humorous, Elegant, etc.)

**OUTPUT JSON:**
```json
{
  "design_style": ["value1", "value2"],
  "color_aesthetic": ["value1", "value2"],
  "placement_display_context": ["value1", "value2"],
  "sentiment_tone": ["value1", "value2"]
}
```
"""

    @lru_cache(maxsize=4)
    def _create_product_classification_prompt(self) -> str:
        """Expert 4: PhÃ¢n loáº¡i sáº£n pháº©m"""
        return """
Báº¡n lÃ  expert PHÃ‚N LOáº I SAN PHáº¨M chi tiáº¿t. HÃ£y táº­p trung vÃ o:

**CHá»ˆ PHÃ‚N TÃCH:**
1. **Niche Theme** (1-2 labels): Chá»§ Ä‘á» (Mother, Father, Police, Beer, Cowgirl, etc.)
2. **Message Type** (1 label): Loáº¡i thÃ´ng Ä‘iá»‡p (No quote, Symbolic Message, etc.)
3. **Personalization Type** (1 label): CÃ¡ nhÃ¢n hÃ³a (Personalized Name, Non-personalized)

**OUTPUT JSON:**
```json
{
  "niche_theme": ["value1", "value2"],
  "message_type": ["value1"],
  "personalization_type": ["value1"]
}
```
"""

    def _get_cached_image(self, image_url: str) -> str:
        """Láº¥y image tá»« cache hoáº·c download vÃ  cache"""
        with self._cache_lock:
            if image_url in self._image_cache:
                return self._image_cache[image_url]

            try:
                # Download image
                response = requests.get(image_url, timeout=10)
                response.raise_for_status()
                image_base64 = base64.b64encode(response.content).decode('utf-8')

                # Cache vá»›i giá»›i háº¡n
                if len(self._image_cache) < 50:  # Giá»›i háº¡n cache
                    self._image_cache[image_url] = image_base64

                return image_base64
            except Exception as e:
                raise Exception(f"Lá»—i download áº£nh: {str(e)}")

    def _analyze_with_expert(self, expert_name: str, image_url: str) -> Dict:
        """PhÃ¢n tÃ­ch vá»›i 1 expert cá»¥ thá»ƒ"""
        try:
            image_base64 = self._get_cached_image(image_url)
            prompt = self.experts[expert_name]()

            # Tá»‘i Æ°u options cho tá»‘c Ä‘á»™
            response = ollama.generate(
                model=self.model,
                prompt=prompt,
                images=[image_base64],
                options={
                    'temperature': 0.1,
                    'top_p': 0.8,
                    'num_ctx': 4096,  # Giáº£m context length
                    'repeat_penalty': 1.05,
                    'num_predict': 512,  # Giáº£m sá»‘ token predict
                    'num_thread': 8,  # TÄƒng sá»‘ thread
                }
            )

            content = response['response']
            json_start = content.find('{')
            json_end = content.rfind('}') + 1

            if json_start != -1 and json_end != -1:
                json_str = content[json_start:json_end]
                result = json.loads(json_str)
                return result
            else:
                print(f"âš ï¸  Expert {expert_name} khÃ´ng tráº£ vá» JSON há»£p lá»‡")
                return {}

        except Exception as e:
            print(f"âš ï¸  Lá»—i expert {expert_name}: {str(e)}")
            return {}

    def analyze_parallel(self, image_url: str, timeout: int = 4) -> Dict:
        """Cháº¡y táº¥t cáº£ experts song song vá»›i timeout"""
        results = {}

        def run_expert(expert_name: str) -> tuple:
            try:
                result = self._analyze_with_expert(expert_name, image_url)
                return expert_name, result
            except Exception as e:
                return expert_name, {}

        # Cháº¡y song song vá»›i ThreadPoolExecutor
        with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
            future_to_expert = {
                executor.submit(run_expert, expert_name): expert_name
                for expert_name in self.experts.keys()
            }

            completed_futures = concurrent.futures.as_completed(
                future_to_expert,
                timeout=timeout
            )

            for future in completed_futures:
                try:
                    expert_name, result = future.result(timeout=1)
                    results.update(result)
                except concurrent.futures.TimeoutError:
                    expert_name = future_to_expert[future]
                    print(f"âš ï¸  Expert {expert_name} timeout")
                except Exception as e:
                    expert_name = future_to_expert[future]
                    print(f"âš ï¸  Expert {expert_name} lá»—i: {str(e)}")

        return results

    def analyze_sequential_fallback(self, image_url: str) -> Dict:
        """Fallback: cháº¡y tuáº§n tá»± náº¿u song song tháº¥t báº¡i"""
        results = {}

        for expert_name in ['basic_info', 'audience_purpose']:  # Chá»‰ cháº¡y 2 experts quan trá»ng nháº¥t
            try:
                result = self._analyze_with_expert(expert_name, image_url)
                results.update(result)
            except Exception as e:
                print(f"âš ï¸  Expert {expert_name} fallback lá»—i: {str(e)}")

        return results


class OptimizedProductPipeline:
    """Pipeline tá»‘i Æ°u chá»‰ sá»­ dá»¥ng GPT-OSS 20B vá»›i MoE"""

    def __init__(self,
                 db_config: Dict[str, str],
                 ollama_model: str = "gpt-oss:20b",
                 milvus_host: str = "10.10.4.25",
                 milvus_port: str = "19530"):
        """
        Khá»Ÿi táº¡o pipeline tá»‘i Æ°u vá»›i MoE

        Args:
            db_config: Cáº¥u hÃ¬nh database PostgreSQL
            ollama_model: Model Ollama (GPT_OSS_20B)
            milvus_host: Milvus host
            milvus_port: Milvus port
        """
        # Database config
        self.db_config = db_config
        self.db_connection = None

        # MoE Labeler
        print("ğŸ”§ Khá»Ÿi táº¡o MoE Labeler...")
        self.moe_labeler = MoELabeler(ollama_model)

        # Khá»Ÿi táº¡o EmbeddingService vá»›i Jina v4
        print("ğŸ”§ Khá»Ÿi táº¡o Jina v4 Embedding Service...")
        self.embedding_service = EmbeddingService()
        self.embedding_dim = self.embedding_service.embedding_dim

        # Milvus config
        self.milvus_host = milvus_host
        self.milvus_port = milvus_port
        self.collection_name = "product_collection_GP_optimized"
        self.collection = None

        # Log info
        model_info = self.embedding_service.get_model_info()
        print(f"ğŸ¤– Embedding Model: {model_info['model_name']}")
        print(f"ğŸ“Š Embedding Dimensions: {model_info['embedding_dimension']}")
        print(f"ğŸ”§ Device: {model_info['device']}")
        print(f"ğŸ¦™ Ollama Model: {ollama_model}")

        # Initialize connections
        self._connect_db()
        self._connect_milvus()
        self._setup_collection()

    def _connect_db(self) -> bool:
        """Káº¿t ná»‘i Ä‘áº¿n PostgreSQL database"""
        try:
            self.db_connection = psycopg2.connect(**self.db_config)
            print("âœ… Káº¿t ná»‘i PostgreSQL thÃ nh cÃ´ng")
            return True
        except Exception as e:
            print(f"âŒ Lá»—i káº¿t ná»‘i database: {e}")
            return False

    def _connect_milvus(self):
        """Káº¿t ná»‘i tá»›i Milvus"""
        try:
            connections.connect(
                alias="default",
                host=self.milvus_host,
                port=self.milvus_port
            )
            print("âœ… Káº¿t ná»‘i Milvus thÃ nh cÃ´ng")
        except Exception as e:
            raise Exception(f"âŒ Lá»—i káº¿t ná»‘i Milvus: {str(e)}")

    def _create_collection_schema(self):
        """Táº¡o schema cho collection"""
        fields = [
            FieldSchema(name="id_sanpham", dtype=DataType.VARCHAR, is_primary=True, auto_id=False, max_length=100),
            FieldSchema(name="image_vector", dtype=DataType.FLOAT_VECTOR, dim=self.embedding_dim),
            FieldSchema(name="description_vector", dtype=DataType.FLOAT_VECTOR, dim=self.embedding_dim),
            FieldSchema(name="image", dtype=DataType.VARCHAR, max_length=1000),
            FieldSchema(name="description", dtype=DataType.VARCHAR, max_length=5000),
            FieldSchema(name="metadata", dtype=DataType.JSON),
            FieldSchema(name="date", dtype=DataType.VARCHAR, max_length=50),
            FieldSchema(name="like", dtype=DataType.VARCHAR, max_length=20),
            FieldSchema(name="comment", dtype=DataType.VARCHAR, max_length=20),
            FieldSchema(name="share", dtype=DataType.VARCHAR, max_length=20),
            FieldSchema(name="link_redirect", dtype=DataType.VARCHAR, max_length=2000),
            FieldSchema(name="platform", dtype=DataType.VARCHAR, max_length=200),
            FieldSchema(name="name_store", dtype=DataType.VARCHAR, max_length=500)
        ]

        schema = CollectionSchema(
            fields=fields,
            description=f"Optimized collection vá»›i embedding {self.embedding_dim}D vÃ  MoE labeling"
        )
        return schema

    def _setup_collection(self):
        """Táº¡o hoáº·c load collection"""
        try:
            if utility.has_collection(self.collection_name):
                self.collection = Collection(self.collection_name)
                print(f"âœ… Load collection '{self.collection_name}' thÃ nh cÃ´ng")
            else:
                schema = self._create_collection_schema()
                self.collection = Collection(self.collection_name, schema)
                self._create_indexes()
                print(f"âœ… Táº¡o collection '{self.collection_name}' thÃ nh cÃ´ng vá»›i {self.embedding_dim}D vectors")

            self.collection.load()

        except Exception as e:
            raise Exception(f"âŒ Lá»—i setup collection: {str(e)}")

    def _create_indexes(self):
        """Táº¡o index cho vector fields"""
        nlist = min(self.embedding_dim, 1024)

        index_params = {
            "metric_type": "COSINE",
            "index_type": "IVF_FLAT",
            "params": {"nlist": nlist}
        }

        self.collection.create_index(
            field_name="image_vector",
            index_params=index_params,
            index_name="image_vector_index"
        )

        self.collection.create_index(
            field_name="description_vector",
            index_params=index_params,
            index_name="description_vector_index"
        )
        print(f"âœ… Táº¡o indexes thÃ nh cÃ´ng vá»›i nlist={nlist}")

    # === DUPLICATE CHECK METHODS ===
    def check_ids_exist_batch(self, id_list: List[str]) -> Dict[str, bool]:
        """Kiá»ƒm tra nhiá»u ID cÃ¹ng lÃºc"""
        try:
            if not id_list:
                return {}

            # Batch check vá»›i chunks Ä‘á»ƒ trÃ¡nh query quÃ¡ lá»›n
            chunk_size = 100
            all_existing_ids = set()

            for i in range(0, len(id_list), chunk_size):
                chunk = id_list[i:i + chunk_size]
                id_conditions = [f'id_sanpham == "{id_val}"' for id_val in chunk]
                expr = " or ".join(id_conditions)

                results = self.collection.query(
                    expr=expr,
                    output_fields=["id_sanpham"],
                    limit=len(chunk)
                )

                chunk_existing = {result["id_sanpham"] for result in results}
                all_existing_ids.update(chunk_existing)

            return {id_val: id_val in all_existing_ids for id_val in id_list}

        except Exception as e:
            print(f"âš ï¸  Lá»—i kiá»ƒm tra batch IDs: {str(e)}")
            return {id_val: False for id_val in id_list}

    def filter_existing_records(self, raw_data_list: List[Dict[str, Any]]) -> tuple:
        """Lá»c bá» cÃ¡c record Ä‘Ã£ tá»“n táº¡i"""
        try:
            if not raw_data_list:
                return [], [], 0

            print(f"ğŸ” Kiá»ƒm tra trÃ¹ng láº·p cho {len(raw_data_list)} records...")

            id_list = [record.get('id_sanpham', '') for record in raw_data_list if record.get('id_sanpham')]
            if not id_list:
                return raw_data_list, [], 0

            existence_map = self.check_ids_exist_batch(id_list)

            new_records = []
            existing_records = []

            for record in raw_data_list:
                id_sanpham = record.get('id_sanpham', '')
                if id_sanpham and existence_map.get(id_sanpham, False):
                    existing_records.append(record)
                else:
                    new_records.append(record)

            duplicate_count = len(existing_records)

            print(f"âœ… Káº¿t quáº£: {len(new_records)} má»›i, {duplicate_count} trÃ¹ng láº·p")
            return new_records, existing_records, duplicate_count

        except Exception as e:
            print(f"âŒ Lá»—i lá»c records: {str(e)}")
            return raw_data_list, [], 0

    # === CRAWL DATA METHODS ===
    def crawl_data_by_date_range(self, start_date: str, end_date: str, limit: int = 1000) -> List[Dict[str, Any]]:
        """Crawl data tá»« database theo khoáº£ng thá»i gian"""
        if not self.db_connection:
            if not self._connect_db():
                return []

        try:
            cursor = self.db_connection.cursor()

            query = """
            SELECT 
                COALESCE(_id, CONCAT('SP_', SUBSTRING(MD5(_id::text), 1, 8))) as id_sanpham,
                COALESCE(original_url, thumb_url, '') as image,
                COALESCE(CAST(created_at_std AS text), CAST(NOW() AS text)) as date,
                COALESCE(CAST("like" AS text), '0') as like,
                COALESCE(CAST(comment AS text), '0') as comment,
                COALESCE(CAST(share AS text), '0') as share,
                COALESCE(final_url, link, '') as link_redirect,
                COALESCE(platform, 'Website') as platform,
                COALESCE(domain, 'unknown') as name_store
            FROM ai_craw.toidispy_full
            WHERE created_at_std BETWEEN %s AND %s
            ORDER BY created_at_std DESC
            LIMIT %s
            """

            cursor.execute(query, (start_date, end_date, limit))
            columns = [desc[0] for desc in cursor.description]
            results = []

            for row in cursor.fetchall():
                row_dict = dict(zip(columns, row))
                results.append(row_dict)

            cursor.close()
            print(f"âœ… Crawl Ä‘Æ°á»£c {len(results)} records tá»« {start_date} Ä‘áº¿n {end_date}")
            return results

        except Exception as e:
            print(f"âŒ Lá»—i crawl data: {e}")
            return []

    # === OPTIMIZED LABELING METHODS ===
    def label_image_fast(self, image_url: str) -> ProductLabel:
        """ÄÃ¡nh label nhanh vá»›i MoE - má»¥c tiÃªu 5 giÃ¢y"""
        start_time = time.time()

        try:
            # Thá»­ cháº¡y song song trÆ°á»›c (4 giÃ¢y timeout)
            result = self.moe_labeler.analyze_parallel(image_url, timeout=4)

            # Náº¿u khÃ´ng Ä‘á»§ data, cháº¡y fallback
            if len(result) < 5:  # Ãt nháº¥t 5 fields
                print("ğŸ”„ Cháº¡y fallback sequential...")
                fallback_result = self.moe_labeler.analyze_sequential_fallback(image_url)
                result.update(fallback_result)

            # Fill missing fields vá»›i defaults
            result = self._fill_missing_fields(result)

            # Táº¡o ProductLabel
            label = ProductLabel(
                image_url=image_url,
                image_recipient=result.get('image_recipient', ['Family']),
                target_audience=result.get('target_audience', ['General']),
                usage_purpose=result.get('usage_purpose', ['Gift']),
                occasion=result.get('occasion', ['Any Occasion']),
                niche_theme=result.get('niche_theme', ['General']),
                sentiment_tone=result.get('sentiment_tone', ['Positive']),
                message_type=result.get('message_type', ['No quote']),
                personalization_type=result.get('personalization_type', ['Non-personalized']),
                product_type=result.get('product_type', ['General Product']),
                placement_display_context=result.get('placement_display_context', ['Home Decor']),
                design_style=result.get('design_style', ['Modern']),
                color_aesthetic=result.get('color_aesthetic', ['Colorful']),
                trademark_level=result.get('trademark_level', 'No TM'),
                main_subject=result.get('main_subject', ['Product']),
                text=result.get('text', ['No text'])
            )

            elapsed_time = time.time() - start_time
            print(f"âš¡ Labeling hoÃ n thÃ nh trong {elapsed_time:.1f}s")

            return label

        except Exception as e:
            elapsed_time = time.time() - start_time
            print(f"âš ï¸  Labeling lá»—i sau {elapsed_time:.1f}s: {str(e)}")

            # Return default label
            return self._create_default_label(image_url)

    def _fill_missing_fields(self, result: Dict) -> Dict:
        """Fill cÃ¡c fields bá»‹ thiáº¿u vá»›i giÃ¡ trá»‹ default"""
        defaults = {
            'image_recipient': ['Family'],
            'target_audience': ['General'],
            'usage_purpose': ['Gift'],
            'occasion': ['Any Occasion'],
            'niche_theme': ['General'],
            'sentiment_tone': ['Positive'],
            'message_type': ['No quote'],
            'personalization_type': ['Non-personalized'],
            'product_type': ['General Product'],
            'placement_display_context': ['Home Decor'],
            'design_style': ['Modern'],
            'color_aesthetic': ['Colorful'],
            'trademark_level': 'No TM',
            'main_subject': ['Product'],
            'text': ['No text']
        }

        for key, default_value in defaults.items():
            if key not in result or not result[key]:
                result[key] = default_value

        return result

    def _create_default_label(self, image_url: str) -> ProductLabel:
        """Táº¡o label default khi lá»—i"""
        return ProductLabel(
            image_url=image_url,
            image_recipient=['Family'],
            target_audience=['General'],
            usage_purpose=['Gift'],
            occasion=['Any Occasion'],
            niche_theme=['General'],
            sentiment_tone=['Positive'],
            message_type=['No quote'],
            personalization_type=['Non-personalized'],
            product_type=['General Product'],
            placement_display_context=['Home Decor'],
            design_style=['Modern'],
            color_aesthetic=['Colorful'],
            trademark_level='No TM',
            main_subject=['Product'],
            text=['No text']
        )

    # === VECTOR GENERATION METHODS ===
    def _generate_vectors(self, text: str, image_url: str = None) -> tuple:
        """Táº¡o embedding vectors cho text vÃ  image"""
        image_vector, text_vector = self.embedding_service._generate_vectors(
            text=text,
            image_url=image_url
        )
        return image_vector, text_vector

    def _create_description(self, label: ProductLabel) -> str:
        """Táº¡o description ngáº¯n gá»n tá»« ProductLabel"""

        def format_list(items: List[str]) -> str:
            if not items:
                return "KhÃ´ng xÃ¡c Ä‘á»‹nh"
            return ", ".join(items[:3])  # Chá»‰ láº¥y 3 items Ä‘áº§u

        # Description ngáº¯n gá»n hÆ¡n Ä‘á»ƒ tÄƒng tá»‘c Ä‘á»™
        description = f"""# {format_list(label.product_type)}

Chá»§ thá»ƒ: {format_list(label.main_subject)}
DÃ nh cho: {format_list(label.image_recipient)}
Dá»‹p: {format_list(label.occasion)}
Phong cÃ¡ch: {format_list(label.design_style)}
MÃ u sáº¯c: {format_list(label.color_aesthetic)}
Text: {format_list(label.text)}

{format_list(label.product_type)} vá»›i thiáº¿t káº¿ {format_list(label.design_style)}, phÃ¹ há»£p cho {format_list(label.occasion)}.
"""
        return description

    # === MAIN PROCESSING METHODS ===
    def process_single_record(self, raw_data: Dict[str, Any]) -> ProductRecord:
        """Xá»­ lÃ½ 1 record vá»›i tá»‘c Ä‘á»™ cao"""
        record_start_time = time.time()

        try:
            image_url = raw_data.get('image', '')
            if not image_url:
                raise Exception("KhÃ´ng cÃ³ URL áº£nh")

            # 1. Label metadata vá»›i MoE (má»¥c tiÃªu 4s)
            label = self.label_image_fast(image_url)
            metadata = asdict(label)

            # 2. Táº¡o description ngáº¯n (má»¥c tiÃªu 0.1s)
            description = self._create_description(label)

            # 3. Táº¡o embedding vectors (má»¥c tiÃªu 0.9s)
            image_vector, description_vector = self._generate_vectors(description, image_url)

            # 4. Táº¡o ProductRecord
            record = ProductRecord(
                id_sanpham=raw_data.get('id_sanpham', f"SP_{uuid.uuid4().hex[:8]}"),
                image_vector=image_vector,
                description_vector=description_vector,
                image=image_url,
                description=description,
                metadata=metadata,
                date=raw_data.get('date', ''),
                like=raw_data.get('like', '0'),
                comment=raw_data.get('comment', '0'),
                share=raw_data.get('share', '0'),
                link_redirect=raw_data.get('link_redirect', ''),
                platform=raw_data.get('platform', ''),
                name_store=raw_data.get('name_store', '')
            )

            elapsed_time = time.time() - record_start_time
            print(f"âš¡ Record processed in {elapsed_time:.1f}s")

            return record

        except Exception as e:
            elapsed_time = time.time() - record_start_time
            raise Exception(
                f"Lá»—i xá»­ lÃ½ record {raw_data.get('id_sanpham', 'unknown')} sau {elapsed_time:.1f}s: {str(e)}")

    def insert_record(self, record: ProductRecord) -> str:
        """Insert 1 ProductRecord vÃ o Milvus"""
        try:
            data = [
                [record.id_sanpham],
                [record.image_vector],
                [record.description_vector],
                [record.image],
                [record.description],
                [record.metadata],
                [record.date],
                [record.like],
                [record.comment],
                [record.share],
                [record.link_redirect],
                [record.platform],
                [record.name_store]
            ]

            mr = self.collection.insert(data)
            self.collection.flush()
            return record.id_sanpham

        except Exception as e:
            raise Exception(f"Lá»—i insert record {record.id_sanpham}: {str(e)}")

    def insert_batch_records(self, records: List[ProductRecord]) -> List[str]:
        """Insert nhiá»u ProductRecord vÃ o Milvus cÃ¹ng lÃºc"""
        try:
            if not records:
                return []

            # Chuáº©n bá»‹ data cho batch insert
            ids = [record.id_sanpham for record in records]
            image_vectors = [record.image_vector for record in records]
            description_vectors = [record.description_vector for record in records]
            images = [record.image for record in records]
            descriptions = [record.description for record in records]
            metadatas = [record.metadata for record in records]
            dates = [record.date for record in records]
            likes = [record.like for record in records]
            comments = [record.comment for record in records]
            shares = [record.share for record in records]
            link_redirects = [record.link_redirect for record in records]
            platforms = [record.platform for record in records]
            name_stores = [record.name_store for record in records]

            data = [
                ids,
                image_vectors,
                description_vectors,
                images,
                descriptions,
                metadatas,
                dates,
                likes,
                comments,
                shares,
                link_redirects,
                platforms,
                name_stores
            ]

            mr = self.collection.insert(data)
            self.collection.flush()

            print(f"âœ… Batch insert thÃ nh cÃ´ng {len(records)} records")
            return ids

        except Exception as e:
            raise Exception(f"Lá»—i batch insert: {str(e)}")

    def run_optimized_pipeline(self, start_date: str, end_date: str,
                               limit: int = 1000,
                               batch_size: int = 5,
                               max_workers: int = 3) -> Dict[str, Any]:
        """
        Cháº¡y pipeline tá»‘i Æ°u vá»›i xá»­ lÃ½ song song cÃ³ giá»›i háº¡n

        Args:
            start_date: NgÃ y báº¯t Ä‘áº§u (YYYY-MM-DD)
            end_date: NgÃ y káº¿t thÃºc (YYYY-MM-DD)
            limit: Sá»‘ lÆ°á»£ng record tá»‘i Ä‘a
            batch_size: Sá»‘ record xá»­ lÃ½ má»—i batch
            max_workers: Sá»‘ thread tá»‘i Ä‘a (khuyáº¿n nghá»‹ 2-3 Ä‘á»ƒ trÃ¡nh quÃ¡ táº£i Ollama)

        Returns:
            Dictionary chá»©a thá»‘ng kÃª káº¿t quáº£
        """
        print("ğŸš€ Báº®T Äáº¦U OPTIMIZED PIPELINE - GPT-OSS 20B + MoE")
        print(f"ğŸ“… Thá»i gian: {start_date} â†’ {end_date}")
        print(f"ğŸ“Š Giá»›i háº¡n: {limit} records, batch_size: {batch_size}, workers: {max_workers}")
        print("-" * 80)

        start_time = time.time()

        # Statistics
        stats = {
            'start_time': datetime.now().isoformat(),
            'crawled_count': 0,
            'duplicate_count': 0,
            'processed_count': 0,
            'inserted_count': 0,
            'failed_count': 0,
            'skipped_duplicates': [],
            'inserted_ids': [],
            'failed_records': [],
            'total_time_seconds': 0,
            'avg_processing_time_per_record': 0,
            'performance_stats': {
                'labeling_times': [],
                'embedding_times': [],
                'insert_times': []
            }
        }

        try:
            # STEP 1: Crawl data tá»« database
            print("ğŸ“¥ STEP 1: Crawl data tá»« database...")
            raw_data_list = self.crawl_data_by_date_range(start_date, end_date, limit)

            if not raw_data_list:
                print("âš ï¸  KhÃ´ng cÃ³ data Ä‘á»ƒ xá»­ lÃ½")
                return stats

            stats['crawled_count'] = len(raw_data_list)
            print(f"âœ… Crawl Ä‘Æ°á»£c {len(raw_data_list)} records")

            # STEP 2: Check duplicates vÃ  lá»c bá»
            print("ğŸ” STEP 2: Kiá»ƒm tra vÃ  lá»c bá» records trÃ¹ng láº·p...")
            new_records, existing_records, duplicate_count = self.filter_existing_records(raw_data_list)

            stats['duplicate_count'] = duplicate_count
            stats['skipped_duplicates'] = [record.get('id_sanpham', 'unknown') for record in existing_records]

            if not new_records:
                print("âš ï¸  Táº¥t cáº£ records Ä‘Ã£ tá»“n táº¡i trong Milvus")
                return stats

            print(f"âœ… Sáº½ xá»­ lÃ½ {len(new_records)} records má»›i")

            # STEP 3: Xá»­ lÃ½ records vá»›i parallel processing cÃ³ giá»›i háº¡n
            print(f"ğŸ”„ STEP 3: Xá»­ lÃ½ vá»›i {max_workers} workers song song...")

            def process_record_wrapper(raw_data: Dict[str, Any]) -> tuple:
                """Wrapper function cho parallel processing"""
                record_id = raw_data.get('id_sanpham', 'unknown')
                try:
                    record_start = time.time()
                    record = self.process_single_record(raw_data)
                    processing_time = time.time() - record_start

                    return 'success', record, record_id, processing_time, None
                except Exception as e:
                    return 'error', None, record_id, 0, str(e)

            # Xá»­ lÃ½ vá»›i ThreadPoolExecutor cÃ³ giá»›i háº¡n workers
            with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
                # Submit táº¥t cáº£ tasks
                future_to_data = {
                    executor.submit(process_record_wrapper, raw_data): raw_data
                    for raw_data in new_records
                }

                # Collect results
                for i, future in enumerate(concurrent.futures.as_completed(future_to_data)):
                    try:
                        status, record, record_id, processing_time, error = future.result()

                        if status == 'success':
                            # Insert record
                            insert_start = time.time()
                            inserted_id = self.insert_record(record)
                            insert_time = time.time() - insert_start

                            stats['processed_count'] += 1
                            stats['inserted_count'] += 1
                            stats['inserted_ids'].append(inserted_id)
                            stats['performance_stats']['labeling_times'].append(processing_time)
                            stats['performance_stats']['insert_times'].append(insert_time)

                            print(f"âœ… [{i + 1}/{len(new_records)}] ThÃ nh cÃ´ng: {record_id} ({processing_time:.1f}s)")

                        else:
                            stats['failed_count'] += 1
                            error_info = {
                                'id_sanpham': record_id,
                                'error': error
                            }
                            stats['failed_records'].append(error_info)
                            print(f"âŒ [{i + 1}/{len(new_records)}] Lá»—i: {record_id} - {error}")

                    except Exception as e:
                        stats['failed_count'] += 1
                        print(f"âŒ [{i + 1}/{len(new_records)}] Lá»—i nghiÃªm trá»ng: {str(e)}")

        except Exception as e:
            print(f"âŒ Lá»—i nghiÃªm trá»ng trong pipeline: {str(e)}")

        finally:
            # TÃ­nh toÃ¡n thá»i gian vÃ  thá»‘ng kÃª
            end_time = time.time()
            stats['total_time_seconds'] = round(end_time - start_time, 2)
            stats['end_time'] = datetime.now().isoformat()

            # TÃ­nh average processing time
            if stats['processed_count'] > 0:
                stats['avg_processing_time_per_record'] = round(
                    sum(stats['performance_stats']['labeling_times']) / stats['processed_count'], 2
                )

            # Log káº¿t quáº£ cuá»‘i cÃ¹ng
            print("=" * 80)
            print("ğŸŠ OPTIMIZED PIPELINE HOÃ€N THÃ€NH!")
            print(f"ğŸ“Š THá»NG KÃŠ Tá»”NG Káº¾T:")
            print(f"   ğŸ“¥ Crawl: {stats['crawled_count']} records")
            print(f"   ğŸ”„ TrÃ¹ng láº·p (bá» qua): {stats['duplicate_count']} records")
            print(f"   ğŸ†• Records má»›i: {len(new_records) if 'new_records' in locals() else 0} records")
            print(f"   ğŸ”„ Xá»­ lÃ½: {stats['processed_count']} records")
            print(f"   âœ… Insert thÃ nh cÃ´ng: {stats['inserted_count']} records")
            print(f"   âŒ Tháº¥t báº¡i: {stats['failed_count']} records")
            print(f"   â±ï¸  Tá»•ng thá»i gian: {stats['total_time_seconds']}s")
            print(f"   ğŸ“ˆ Avg thá»i gian/record: {stats['avg_processing_time_per_record']}s")

            # Performance analysis
            if stats['performance_stats']['labeling_times']:
                avg_labeling = sum(stats['performance_stats']['labeling_times']) / len(
                    stats['performance_stats']['labeling_times'])
                max_labeling = max(stats['performance_stats']['labeling_times'])
                min_labeling = min(stats['performance_stats']['labeling_times'])
                print(
                    f"   ğŸ·ï¸  Labeling time - Avg: {avg_labeling:.1f}s, Min: {min_labeling:.1f}s, Max: {max_labeling:.1f}s")

            # TÃ­nh tá»‰ lá»‡ thÃ nh cÃ´ng
            new_records_count = len(new_records) if 'new_records' in locals() else max(
                stats['crawled_count'] - stats['duplicate_count'], 1)
            success_rate = stats['inserted_count'] / max(new_records_count, 1) * 100
            print(f"   ğŸ“ˆ Tá»‰ lá»‡ thÃ nh cÃ´ng: {success_rate:.1f}%")

            # Performance target check
            target_time = 5.0  # 5 seconds per record target
            if stats['avg_processing_time_per_record'] <= target_time:
                print(f"   ğŸ¯ Má»¤C TIÃŠU Äáº T: {stats['avg_processing_time_per_record']:.1f}s â‰¤ {target_time}s")
            else:
                print(f"   âš ï¸  Má»¤C TIÃŠU CHÆ¯A Äáº T: {stats['avg_processing_time_per_record']:.1f}s > {target_time}s")

            print("=" * 80)

            return stats

    def search_similar_products(self, query_vector: List[float],
                                field_name: str = "image_vector",
                                top_k: int = 5):
        """TÃ¬m kiáº¿m sáº£n pháº©m tÆ°Æ¡ng tá»±"""
        search_params = {
            "metric_type": "COSINE",
            "params": {"nprobe": 10}
        }

        results = self.collection.search(
            data=[query_vector],
            anns_field=field_name,
            param=search_params,
            limit=top_k,
            output_fields=["id_sanpham", "image", "platform", "metadata"]
        )

        return results

    def save_stats_to_json(self, stats: Dict[str, Any], filename: str = None):
        """LÆ°u thá»‘ng kÃª vÃ o file JSON"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"optimized_pipeline_stats_{timestamp}.json"

        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(stats, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ ÄÃ£ lÆ°u thá»‘ng kÃª vÃ o: {filename}")
        except Exception as e:
            print(f"âŒ Lá»—i lÆ°u file: {e}")

    def close_connections(self):
        """ÄÃ³ng táº¥t cáº£ káº¿t ná»‘i"""
        try:
            if self.db_connection:
                self.db_connection.close()
                print("âœ… ÄÃ£ Ä‘Ã³ng káº¿t ná»‘i PostgreSQL")
        except:
            pass

        # Clear image cache in MoE labeler
        try:
            self.moe_labeler._image_cache.clear()
            print("âœ… ÄÃ£ clear image cache")
        except:
            pass

def main():
    """
    HÃ m main tá»‘i Æ°u - chá»‰ GPT-OSS 20B vá»›i MoE
    Má»¥c tiÃªu: 5 giÃ¢y/record
    """
    print("ğŸš€ KHá»I Äá»˜NG OPTIMIZED PIPELINE - GPT-OSS 20B + MoE")
    print("ğŸ¯ Má»¥c tiÃªu: 5 giÃ¢y/record")
    print("=" * 60)

    # ========== CONFIGURATION ==========
    # Database config
    db_config = {
        'host': '45.79.189.110',
        'database': 'ai_db',
        'user': 'ai_engineer',
        'password': 'StrongPassword123',
        'port': 5432
    }

    # Ollama config - Chá»‰ GPT_OSS_20B
    ollama_model = "gpt-oss:20b"

    # Milvus config
    milvus_host = "10.10.4.25"
    milvus_port = "19530"

    # ========== THá»œI GIAN CRAWL ==========
    start_date = "2024-08-12"  # YYYY-MM-DD
    end_date = "2025-08-12"  # YYYY-MM-DD

    # ========== CÃ€I Äáº¶T PIPELINE Tá»I Æ¯U ==========
    limit = 100  # Giá»›i háº¡n sá»‘ record Ä‘á»ƒ test performance
    batch_size = 5  # Batch size nhá» hÆ¡n Ä‘á»ƒ kiá»ƒm soÃ¡t memory
    max_workers = 2  # Giá»›i háº¡n workers Ä‘á»ƒ trÃ¡nh quÃ¡ táº£i Ollama

    try:
        # Khá»Ÿi táº¡o pipeline
        print("ğŸ”§ Khá»Ÿi táº¡o optimized pipeline...")
        pipeline = OptimizedProductPipeline(
            db_config=db_config,
            ollama_model=ollama_model,
            milvus_host=milvus_host,
            milvus_port=milvus_port
        )

        print("âœ… Pipeline khá»Ÿi táº¡o thÃ nh cÃ´ng!")

        # Performance check trÆ°á»›c khi cháº¡y
        print("ğŸ§ª Äang test performance vá»›i 1 record máº«u...")
        test_data = [{'id_sanpham': 'TEST_001', 'image': 'https://example.com/test.jpg'}]
        test_start = time.time()

        # Bá» qua test náº¿u khÃ´ng cÃ³ image URL thá»±c
        print("âš ï¸  Bá» qua test performance, cháº¡y pipeline thá»±c...")

        # Cháº¡y pipeline chÃ­nh
        print(f"ğŸ¯ Báº¯t Ä‘áº§u crawl data tá»« {start_date} Ä‘áº¿n {end_date}")

        stats = pipeline.run_optimized_pipeline(
            start_date=start_date,
            end_date=end_date,
            limit=limit,
            batch_size=batch_size,
            max_workers=max_workers
        )

        # LÆ°u thá»‘ng kÃª
        pipeline.save_stats_to_json(stats)

        # Performance report
        print("\nğŸŠ BÃO CÃO PERFORMANCE:")
        print(f"âœ… ThÃ nh cÃ´ng: {stats['inserted_count']}/{stats['crawled_count']} records")
        print(f"ğŸ”„ TrÃ¹ng láº·p: {stats['duplicate_count']} records")
        print(f"â±ï¸  Avg time/record: {stats['avg_processing_time_per_record']}s")
        print(f"ğŸ¯ Target: 5.0s/record")

        if stats['avg_processing_time_per_record'] <= 5.0:
            print("ğŸ‰ Äáº T Má»¤C TIÃŠU PERFORMANCE!")
        else:
            print("âš ï¸  CHÆ¯A Äáº T Má»¤C TIÃŠU - Cáº§n tá»‘i Æ°u thÃªm")

        # Äá» xuáº¥t tá»‘i Æ°u
        if stats['avg_processing_time_per_record'] > 5.0:
            print("\nğŸ’¡ Äá»€ XUáº¤T Tá»I Æ¯U:")
            print("   - Giáº£m context length trong Ollama options")
            print("   - TÄƒng num_thread trong Ollama")
            print("   - Giáº£m sá»‘ experts trong MoE")
            print("   - Sá»­ dá»¥ng image cache hiá»‡u quáº£ hÆ¡n")
            print("   - Giáº£m timeout cho parallel execution")

    except Exception as e:
        print(f"âŒ Lá»–I NGHIÃŠM TRá»ŒNG: {str(e)}")

    finally:
        try:
            pipeline.close_connections()
        except:
            pass

        print("\nğŸ‘‹ Optimized Pipeline káº¿t thÃºc!")


if __name__ == "__main__":
    main()